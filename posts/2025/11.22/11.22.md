## Editing Across Languages: A Survey of Multilingual Knowledge Editing

首次全面调查和方法比较研究以及多语言知识编辑的基准。

## 基于的方法

#### parameter editing methods

+ General KE Methods Adapted for MKE
  + ROME
  + MEMIT
  + KnowledgeNeuron
  + PMET

+ Specific Methods Designed for MKE
  + MEMAT
  + MPN
  + LU-LAFNs
  + MEMLA

### Memory-based methods

+ In-context editing
+ Memory-retriever Editing
  + SERAC
  + ReMaKE
  + MQAKEAL
  + Mello-CL

### Fine-tuning methods

+ Baseline Fine-Tuning Methods
  + Full-fine tuning
  + LoRA-FT
  + FT-L
  + FT-M
+ Instruction-Tuned Knowledge Editing
  + LTE
  + X-KDE

### Hypernetwork-Based Methods

+ MEND
+ KnowledgeEditor
+ LiME



Multilingual knowledge editing (MKE) has emerged as a promising extension of the model editing paradigm, enabling factual updates that generalize across languages. While recent progress has introduced diverse method families, no single method fully solves the core challenge of reliable multi-lingual propagation. Parameter-based methods are limited by representation anisotropy; memorybased methods face retrieval bottlenecks and storage overhead; prompting is unreliable and fragile across languages; fine-tuning is static and prone to interference; and hypernetwork approaches, though promising, remain underexplored. These limitations motivate several critical open challenges.

### Open challenges

+ Language Anisotropy and Representation Misalignment 

+ Lack of Evaluation Coverage for Low-Resource Languages 

+ Trade-off Between Locality and Propagation

+ Benchmark Fragmentation and Lack of Standardization

+ Scalability and Real-World Applicability

  

### Opportunities

+ Language-Conditioned or Language-Aware Editing 
+ Instruction-Tuned and Task-Oriented Editing
+ Multilingual In-Context Learning and Adapter Fusion
+ Development of Robust Multilingual Benchmarks
+ Cross-Model Editing and Transfer 

