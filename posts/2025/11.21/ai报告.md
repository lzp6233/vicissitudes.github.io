

# 前沿大模型遗忘（Unlearning）与编辑（Editing）技术及测评体系深度研究报告





## 执行摘要



随着大语言模型（LLM）参数规模的指数级增长，其作为知识库的潜力已得到广泛认可。然而，这种静态的知识存储方式在面对现实世界动态变化的信息环境时，暴露出了显著的滞后性与刚性。当模型内部包含过时信息、错误事实、隐私数据或有害内容时，传统的全量重新训练（Retraining）因其高昂的计算成本（通常需数月时间和数百万美元的算力投入）而变得不可行。因此，如何在不破坏模型原有能力的前提下，精准地擦除特定数据影响（Machine Unlearning，机器遗忘）或修正特定事实（Model Editing，模型编辑），已成为当前人工智能安全与对齐领域的必争之地。

本报告旨在对2024年至2025年间涌现的前沿技术进行详尽的梳理与深度剖析。我们将深入探讨从基于梯度的负向偏好优化（NPO）到无参考模型的SimNPO技术，解析它们如何通过精细操纵损失景观来实现对特定记忆的“手术式”移除。同时，报告将详细剖析以ROME、MEMIT为代表的定位-编辑（Locate-then-Edit）范式，以及PMET和MEND等元学习与超网络方法如何试图突破“可靠性-泛化性-局部性”的不可能三角。此外，本报告特别关注以TOFU、KnowEdit和R-TOFU为代表的新一代测评基准，剖析从传统的准确率指标向成员推理攻击（MIA）、Forget Quality（FQ）等概率统计指标演进的深层逻辑。



## 1. 机器遗忘（Machine Unlearning）：从破坏到对齐的范式转移



机器遗忘在LLM语境下被定义为一种后训练（Post-training）过程，旨在从预训练模型中移除特定训练样本集（Forget Set）的影响，使得处理后的模型在统计学上无法与从未见过该数据的模型区分开来，同时保留对剩余数据（Retain Set）的效用。早期的研究多集中于简单的破坏性遗忘，但随着安全对齐需求的提升，该领域正经历从“破坏”到“对齐”的深刻范式转移。



### 1.1 梯度上升与早期的探索



在LLM遗忘技术的早期探索阶段，最直观的思路是逆转训练过程。既然训练是通过梯度下降（Gradient Descent）最小化预测误差，那么遗忘似乎可以通过梯度上升（Gradient Ascent, GA）来实现。GA的核心思想是最大化模型在遗忘集上的损失函数 $\mathcal{L}$。



#### 1.1.1 梯度上升（GA）的理论困境与模型崩溃



形式上，如果参数更新规则为 $\theta_{t+1} = \theta_t + \eta \nabla \mathcal{L}(D_f)$，其中 $\eta$ 是学习率，$D_f$ 是遗忘集，理论上模型会逐渐“远离”这些数据的最优解。然而，实际操作中GA面临着严重的数值稳定性问题。LLM的损失景观是高度非凸的，简单的梯度上升往往会将参数推向损失函数的平坦区域或极值点，这些区域往往对应着参数空间的“荒原”，导致模型输出崩塌（Model Collapse）。具体表现为，模型不仅忘记了目标数据，甚至丧失了基本的语言生成能力，输出变成无意义的乱码。



#### 1.1.2 梯度差分（Gradient Difference, GD）的尝试



为了缓解模型崩溃，研究者引入了保留集（Retain Set）作为锚点，提出了梯度差分（GD）方法。GD试图在最大化遗忘集损失的同时，最小化保留集的损失：



$$\mathcal{L}_{GD} = -\mathcal{L}_{forget} + \alpha \mathcal{L}_{retain}$$



这里的超参数 $\alpha$ 用于平衡遗忘与保留的权重。虽然GD在一定程度上缓解了灾难性遗忘，但其本质上的对抗性目标（一个推，一个拉）导致了优化过程的极度不稳定。当 $\alpha$ 设置不当时，模型要么无法有效遗忘（保留集梯度占主导），要么仍然发生能力退化（遗忘集梯度导致参数漂移过远）。此外，GD方法对计算资源的需求较高，因为每一步更新都需要计算保留集的梯度，这在处理大规模模型时效率低下 1。



### 1.2 负向偏好优化（NPO）：遗忘即对齐



2024年，随着直接偏好优化（DPO）在RLHF（基于人类反馈的强化学习）中的成功，研究人员开始尝试将遗忘问题重构为偏好对齐问题。如果说训练是让模型“喜欢”正确的数据，那么遗忘就是让模型“厌恶”特定的数据。这一思想催生了负向偏好优化（Negative Preference Optimization, NPO）。



#### 1.2.1 NPO的机制解析



NPO不仅仅是简单的梯度上升，它引入了一个参考模型（Reference Model，通常是原始的预训练模型）来约束参数的更新幅度。NPO将遗忘集中的样本视为DPO框架中的“拒绝”（Rejected）样本，而通常不设定明确的“接受”样本（或者使用空/中性回答）。NPO的损失函数设计精妙地利用了KL散度约束，使得模型在降低遗忘样本生成概率的同时，不会偏离原始模型太远。

$$ \mathcal{L}*{NPO} = - \log \sigma \left( \beta \log \frac{\pi*{\theta}(y_{forget}|x)}{\pi_{ref}(y_{forget}|x)} - \beta \log \frac{\pi_{\theta}(y_{neutral}|x)}{\pi_{ref}(y_{neutral}|x)} \right) $$

在这个公式中，$\beta$ 是控制KL惩罚强度的温度参数。NPO的优势在于其稳定性：它为遗忘过程提供了一个“软着陆”的机制。通过限制模型与参考模型的偏离程度，NPO有效地防止了模型崩溃，并在TOFU等基准测试中展现了优于GA和GD的性能 1。



#### 1.2.2 NPO的局限性：参考模型偏差



尽管NPO取得了显著进展，但深入的分析揭示了其内在缺陷——参考模型偏差（Reference Model Bias）。在NPO中，遗忘的梯度信号强度依赖于当前模型与参考模型的概率比值。然而，参考模型正是那个“记住了”遗忘数据的模型，因此 $\pi_{ref}(y_{forget}|x)$ 的值通常很高。这导致在优化初期，梯度信号可能被抑制，或者在面对不同难度的遗忘样本时（例如，有些样本是强记忆的，有些是弱记忆的），NPO难以自适应地分配优化力度。强记忆样本往往因为参考模型的高置信度而变得难以被“撬动”，导致遗忘不彻底。



### 1.3 SimNPO：去繁就简的无参考优化



针对NPO的痛点，2024年末提出的SimNPO技术代表了该领域的最新SOTA（State-of-the-Art）。SimNPO的核心洞察是：在遗忘任务中，依赖一个包含有害知识的参考模型可能是有害的。



#### 1.3.1 SimNPO的理论突破



SimNPO移除了NPO损失函数中的 $\pi_{ref}$ 项，转而使用一个简单的偏好优化框架。这种“无参考”（Reference-Free）的设计不仅极大地降低了计算成本（不再需要前向传播参考模型），更重要的是，它消除了参考模型对优化的潜在阻碍。

SimNPO引入了长度归一化（Length Normalization）机制来替代参考模型的隐式归一化作用。由于不同的回答长度差异巨大，直接优化概率会导致模型倾向于生成极短的回答（因为长序列的联合概率天然更低）。SimNPO通过显式的长度惩罚或归一化，确保模型是在针对内容进行遗忘，而不是仅仅学会了“闭嘴”。



#### 1.3.2 实证优势：TOFU基准上的表现



在TOFU（Task of Fictitious Unlearning）基准测试中，SimNPO展现了压倒性的优势。特别是在Forget Quality（FQ）指标上，SimNPO生成的数据分布与全量重新训练（Retrain）的模型更为接近。

- **处理不均衡难度：** NPO往往在强记忆样本上表现不佳，而SimNPO能够更均匀地处理不同记忆强度的样本，确保“难啃的骨头”也能被有效遗忘。
- **计算效率：** 由于省去了参考模型的加载和计算，SimNPO在训练速度和显存占用上均优于NPO，使其更适合在资源受限的环境下部署 1。



### 1.4 向量算术与表示工程：非参数化的遗忘路径



除了直接修改模型权重，另一条技术路线关注于模型内部的表示空间和权重向量空间，这类方法通常被称为“引导”（Steering）或“向量算术”。



#### 1.4.1 任务向量（Task Vectors）与算术操作



任务向量是基于参数空间线性特性的发现。研究表明，微调后的模型权重 $\theta_{ft}$ 与预训练权重 $\theta_{pre}$ 之间的差值 $\tau = \theta_{ft} - \theta_{pre}$ 代表了特定任务的“方向”。

遗忘可以通过向量减法来实现：$\theta_{unlearn} = \theta_{ft} - \lambda \tau_{forget}$。

- **缩放因子 $\lambda$ 的艺术：** 简单的减法（$\lambda=1$）往往效果不佳。研究发现，$\lambda$ 起到了控制遗忘强度的作用。过小的 $\lambda$ 导致遗忘残留，而过大的 $\lambda$ 则会导致“反向知识”（Antonym Generation），即模型不仅忘记了“A是B”，反而坚信“A不是B”或“A是C（错误答案）”。寻找最优的 $\lambda$ 通常需要基于验证集的搜索，这被称为“控制向量缩放”技术 7。
- **局限性：** 任务向量方法假设参数空间是局部线性的，这在深层非线性网络中并不总是成立。因此，这种方法虽然极其高效（无需训练），但在精度上往往不如基于优化的方法。



#### 1.4.2 RMU（Representation Misdirection for Unlearning）



RMU技术不直接修改权重以最小化生成的概率，而是试图扰动模型对特定概念的内部激活（Activation）。

- **机制：** RMU在模型的中间层（通常是第N层）寻找遗忘概念的激活向量，并训练一个“转向向量”或微调权重，使得当模型遇到该概念时，其激活状态被“误导”向一个随机或中性的方向。
- **层级敏感性：** RMU的效果高度依赖于选择哪一层进行扰动。浅层扰动可能破坏基础特征提取，深层扰动可能无效。最新的“自适应RMU”（Adaptive RMU）尝试自动学习每一层的扰动系数，以实现更精细的控制 9。



### 1.5 上下文感知与推理遗忘：R-TOFU的启示



随着推理模型（如OpenAI o1系列）的兴起，简单的答案层面的遗忘已不足以满足安全需求。



#### 1.5.1 链式思维（CoT）中的残留



R-TOFU基准的研究揭示了一个惊人的现象：即使模型在最终答案中拒绝了敏感信息（输出“我不知道”），其内部生成的思维链（Chain of Thought, CoT）中仍可能包含完整的敏感推理过程。例如，在制造生化武器的查询中，模型可能在CoT中详细列出了化学配方，但在最后一步输出“由于安全策略，我无法回答”。

- **隐性风险：** 这种“假性遗忘”极具危险性，因为攻击者可以通过解码策略（如强制输出CoT）或侧信道攻击提取出这些残留知识。



#### 1.5.2 Reasoned IDK策略



针对这一问题，**Reasoned IDK** 策略被提出。它不仅要求模型输出拒绝，还要求模型生成一段“连贯的、解释为何不知道或为何拒绝”的推理链。通过在CoT层面进行偏好优化（DPO/SimNPO），确保遗忘深入到推理过程的每一个环节，而不仅仅是最终的Token 11。



## 2. 模型编辑（Model Editing）：精准的手术刀



如果说机器遗忘是大面积的“清洗”，那么模型编辑就是精准的“微创手术”。模型编辑的目标是在不重新训练模型的情况下，修正特定的事实错误或注入新的知识（例如：“英国首相”从“苏纳克”变为“斯塔默”）。



### 2.1 定位-编辑（Locate-then-Edit）范式



这一范式基于“知识神经元”假设，认为LLM中的事实知识存储在特定的神经元或层中。



#### 2.1.1 ROME（Rank-One Model Editing）



ROME是该领域的奠基之作。它将Transformer的前馈神经网络（FFN）视为键值对（Key-Value）存储器。

- **因果追踪（Causal Tracing）：** ROME首先通过因果中介分析，定位出对特定事实预测贡献最大的FFN层。
- **秩-1更新：** 假设我们要将知识 $(s, r, o)$ 修改为 $(s, r, o^*)$。ROME计算一个秩-1矩阵更新 $\Delta W$，使得输入主语 $s$ 的向量表示 $k$ 经过 $W + \Delta W$ 变换后，能输出新的目标向量 $v^*$。公式上，$W_{new} = W_{old} + \Lambda (C^{-1} k^T)$，其中 $C$ 是协方差矩阵，用于保护未修改的知识。
- **局限：** ROME一次只能编辑一个事实，且难以处理批量更新 12。



#### 2.1.2 MEMIT（Mass-Editing Memory in a Transformer）



为了解决ROME的扩展性问题，MEMIT应运而生。MEMIT不再局限于单层更新，而是将更新“分摊”到多个层中。

- **最小二乘法：** MEMIT将批量编辑问题建模为一个大规模的最小二乘问题。它寻找一组参数更新，使得在数千个新的键值对上误差最小，同时尽可能保持对旧知识的拟合。
- **性能：** MEMIT能够一次性注入上万条知识，且保持较高的模型效用，是目前工业界应用最广泛的编辑算法之一 13。



#### 2.1.3 PMET（Pliable Model Editing in Transformers）



PMET是对MEMIT的进一步理论修正。PMET的研究者指出，ROME和MEMIT主要关注FFN，忽略了多头自注意力（MHSA）机制在知识检索中的作用。

- **双组件优化：** PMET同时优化MHSA和FFN的隐藏状态。它认为MHSA负责将信息“路由”到正确的FFN神经元。如果只修改FFN，MHSA可能会将查询路由到错误的“地址”。
- **精度提升：** 通过同时调整这两个组件，PMET在Locality（局部性，即不影响无关知识）和Portability（可移植性，即对不同问法的适应性）上均超越了MEMIT 13。



### 2.2 元学习（Meta-Learning）与超网络（Hypernetworks）



这一类方法试图通过“学习如何编辑”来解决编辑问题。



#### 2.2.1 MEND（Model Editor Networks with Gradient Decomposition）



MEND不直接计算解析解，而是训练一个辅助的神经网络（超网络）。

- **梯度分解：** MEND接受标准微调产生的梯度作为输入，并将其分解为低秩形式。超网络学习将这些原始梯度转化为“更安全、更有效”的权重更新 $\Delta \theta$。
- **速度优势：** 一旦训练完成，MEND在推理阶段极快，不需要像ROME那样进行复杂的逆矩阵计算。它特别适合超大规模模型（如GPT-3, Llama-405B），因为其计算复杂度主要取决于梯度的秩，而非模型参数总量 15。



#### 2.2.2 MALMEN



MALMEN进一步扩展了MEND的思想，利用正规方程（Normal Equations）来聚合参数偏移。它解决了MEND在处理大规模批量编辑时的内存瓶颈，使得超网络方法也能像MEMIT一样处理成千上万的并发编辑请求 15。



### 2.3 终身编辑（Lifelong Editing）与WISE架构



当前的编辑方法大多假设“一次性”场景。但在现实中，知识库是不断流动的，模型需要经历成千上万次连续的编辑。

- **灾难性遗忘的再现：** 研究发现，连续使用ROME或MEMIT进行编辑，会导致模型参数逐渐漂移，最终导致“模型痴呆”（Model Dementia），即模型丧失了推理能力或忘记了最早期的编辑。
- **WISE（NeurIPS 2024）：** WISE提出了一种**双存储器架构**来解决这个问题。
  - **主存储器（Main Memory）：** 冻结的预训练参数，负责通用推理和基础知识。
  - **侧存储器（Side Memory）：** 一个可学习的、参数量较小的模块，专门用于存储新的编辑。
  - **路由与分片（Routing & Sharding）：** 一个路由模块决定查询是走主存储器还是侧存储器。侧存储器被进一步分片，不同的编辑存储在不同的子空间中，互不干扰。这种设计从架构层面规避了稳定性-可塑性困境（Stability-Plasticity Dilemma），代表了未来长效模型维护的方向 16。



### 2.4 正则化技术：RECT



在追求编辑成功率的同时，很多方法忽略了对模型通用能力的损害。RECT（RElative Change in weighT）作为一种正则化技术被提出。

- **机制：** RECT在损失函数中加入了一个约束项，惩罚那些对模型通用能力至关重要的权重的变化。它利用Fisher信息矩阵或其他重要性度量，识别出“关键参数”，并迫使编辑操作绕开这些参数，或者寻找正交的更新方向。实验证明，RECT能在保持94%编辑成功率的同时，显著减少对推理能力的副作用 18。



## 3. 测评体系：从定性到定量的飞跃



2024-2025年间，该领域最大的进步之一是建立了严格、标准化的测评基准。



### 3.1 TOFU：机器遗忘的黄金标准



TOFU（Task of Fictitious Unlearning）基准通过构建完全虚构的作者和传记，解决了“预训练知识泄露”的难题。因为这些知识在预训练数据中不存在，所以我们能确信模型对这些知识的掌握完全来自于微调阶段。



#### 3.1.1 数据集构成



TOFU包含200个由GPT-4生成的虚构作者档案，每个档案包含20个问答对。

- **遗忘集（Forget Set）：** 目标被移除的作者（如1%、5%、10%）。
- **保留集（Retain Set）：** 需要保留的其他虚构作者。
- **真实世界集（Real World）：** 真实存在的作者和通用知识，用于检测遗忘是否“误伤”了预训练知识。



#### 3.1.2 核心指标



- **Forget Quality (FQ)：** 这是TOFU最核心的创新。它不是简单看准确率是否下降，而是使用统计检验（如Kolmogorov-Smirnov Test）来比较“遗忘后模型”与“重训练模型（Retrain）”在遗忘集上的输出分布。理想的FQ应接近1，意味着两者不可区分。
- **Truth Ratio：** 计算模型生成正确答案的概率与生成扰动后错误答案概率的比值。这一指标比单纯的概率更能反映模型内部的知识倾向。



### 3.2 KnowEdit：全方位的编辑基准



KnowEdit整合了ZsRE、WikiBio、ConvSent等多个数据集，提供了一个多维度的评估视角。



#### 3.2.1 评估维度



- **Edit Success (Reliability)：** 编辑是否成功？
- **Portability (Generalization)：** 编辑是否能泛化？（例如，教了“A是B的父亲”，问“B的父亲是谁”能否回答A）。
- **Locality (Specificity)：** 是否影响了无关知识？（例如，改了“A的生日”，不能影响“A的出生地”）。
- **Fluency：** 生成文本是否通顺？



### 3.3 隐私与成员推理攻击（MIA）



在隐私合规场景下，仅靠FQ是不够的。MIA攻击试图判断某个样本是否在训练集中。

- **LiRA (Likelihood Ratio Attack)：** 当前最强的MIA攻击。它训练多个影子模型（Shadow Models）来估计样本的基准损失分布。如果目标模型对某个遗忘样本的损失显著低于基准，说明遗忘不彻底。
- **Min-k%：** 关注序列中概率最低的k%个Token。如果这些“难”Token的概率异常高，往往是记忆残留的铁证。TOFU排行榜显示，很多声称“遗忘成功”的方法在LiRA面前都会原形毕露 20。



## 4. 数据透视与对比分析



为了更直观地展示各技术的优劣，我们整理了以下对比表格。



### 表1：主流LLM遗忘技术深度对比



| **技术名称**                 | **核心范式** | **机制原理**                       | **优势**                           | **劣势**                         | **最佳适用场景**    | **TOFU表现 (FQ)** |
| ---------------------------- | ------------ | ---------------------------------- | ---------------------------------- | -------------------------------- | ------------------- | ----------------- |
| **Gradient Ascent (GA)**     | 优化         | 最大化遗忘集损失                   | 实现简单，直观                     | 极易导致模型崩溃，损失语言能力   | 无（基线方法）      | 极低              |
| **Gradient Difference (GD)** | 优化         | 遗忘集上升 + 保留集下降            | 引入保留集约束，缓解崩溃           | 优化目标对抗，超参难调，不稳定   | 简单场景            | 低                |
| **NPO**                      | 偏好优化     | 负向DPO，引入参考模型              | 稳定，防止参数大幅漂移             | 参考模型偏差导致强记忆难消除，慢 | 通用遗忘            | 中高              |
| **SimNPO**                   | 偏好优化     | 无参考模型，长度归一化             | **SOTA**，高效，无偏差，自适应难度 | 对超参 $\beta$ 敏感              | 高精度隐私删除      | **最高**          |
| **Task Vectors**             | 向量算术     | 权重减法 ($\theta - \lambda \tau$) | 推理期干预，无训练成本             | 容易产生“反义”而非“无知”，精度低 | 临时屏蔽，毒性消除  | 中                |
| **RMU**                      | 表示工程     | 激活层扰动 (Steering)              | 精准打击特定概念                   | 层级选择困难，需白盒访问         | 特定概念移除 (如HP) | 中                |
| **In-Context**               | 推理提示     | 系统提示词 (System Prompt)         | 零成本，即插即用                   | 浅层遗忘，易被越狱 (Jailbreak)   | 临时合规，轻量级    | 不适用            |



### 表2：主流模型编辑技术深度对比



| **技术名称** | **类型**  | **更新范围** | **记忆机制**       | **扩展性 (Scalability)** | **局部性 (Locality)** | **终身编辑能力** |
| ------------ | --------- | ------------ | ------------------ | ------------------------ | --------------------- | ---------------- |
| **ROME**     | 定位-编辑 | 单层 FFN     | 秩-1 更新          | 低 (1次编辑)             | 高                    | 极差             |
| **MEMIT**    | 定位-编辑 | 多层 FFN     | 最小二乘法         | 高 (10k+ 编辑)           | 高                    | 差               |
| **PMET**     | 定位-编辑 | MHSA + FFN   | 注意力+FFN联合优化 | 高                       | **极高**              | 差               |
| **MEND**     | 元学习    | 超网络预测   | 梯度分解           | **极高** (推理极快)      | 中                    | 中               |
| **WISE**     | 架构改进  | 侧存储器     | 路由检索           | 无限 (理论上)            | 高                    | **极高**         |



### 表3：测评基准特性一览



| **基准名称** | **领域** | **数据集构成**            | **核心创新点**               | **关键指标**                       |
| ------------ | -------- | ------------------------- | ---------------------------- | ---------------------------------- |
| **TOFU**     | 遗忘     | 虚构作者问答 (GPT-4生成)  | 隔离预训练知识，纯净评估环境 | Forget Quality (FQ), Truth Ratio   |
| **KnowEdit** | 编辑     | ZsRE, WikiBio, ConvSent   | 多任务聚合，全面评估         | Reliability, Portability, Locality |
| **R-TOFU**   | 推理遗忘 | 合成CoT轨迹               | 关注推理过程中的知识残留     | CoT Forget Efficacy                |
| **MUSE**     | 遗忘     | 新闻、书籍 (Harry Potter) | 真实世界版权数据，自然语料   | Verbatim Overlap, MIA Scores       |



## 5. 深度洞察与战略启示





### 5.1 “不可能三角”的博弈



在模型编辑与遗忘领域，存在一个著名的“不可能三角”：**可靠性（Reliability）**、**局部性（Locality）\**和\**泛化性（Generalization）**。

- **NPO/SimNPO** 等遗忘方法在可靠性（彻底删除）上表现出色，但往往牺牲局部性（误伤保留集）。
- **ROME/PMET** 等编辑方法在局部性上做到了极致（只改一点），但泛化性往往受限（换个问法就不会了）。
- 未来的突破方向在于**打破单体模型的桎梏**。WISE的成功表明，将“知识存储”与“推理计算”解耦，采用模块化的记忆架构，可能是解决这一矛盾的终极方案。



### 5.2 从“防御”到“合规”



随着《人工智能法案》（EU AI Act）等法规的落地，遗忘技术不再仅仅是技术防御手段，更是法律合规的必需品。TOFU基准中引入的 LiRA 和 Min-k% 等对抗性指标表明，未来的合规标准将非常严苛：仅仅“不说”是不够的，必须证明数据在统计学层面上“不存在”。这将推动企业从简单的提示词工程转向 SimNPO 等深度优化技术。



### 5.3 推理遗忘的新挑战



R-TOFU的研究敲响了警钟。随着Agent和CoT的普及，模型内部的思维过程变得透明。如果模型在“心里”默念了有害信息，即使嘴上不说，也构成了巨大的安全隐患。这意味着未来的遗忘技术必须深入到 Transformer 的中间层激活和注意力机制中，进行更深层次的“思维清洗”。



## 6. 结论



2025年的大模型遗忘与编辑技术正处于从“实验性探索”向“工业级应用”跨越的关键期。**SimNPO** 确立了遗忘任务的新基线，证明了无参考优化的有效性；**PMET** 和 **WISE** 则展示了模型编辑向多组件协同和终身学习方向演进的趋势。然而，面对推理残留、跨语言泛化和严格的隐私合规挑战，现有的技术栈仍需迭代。对于研究人员和从业者而言，关注**模块化架构**、**推理链遗忘**以及**对抗性评估**将是未来破局的关键。

------

*(报告结束)*