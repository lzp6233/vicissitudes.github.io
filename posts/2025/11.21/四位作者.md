

# 大型语言模型机器遗忘（LLM Unlearning）的前沿方法与战略范式：2023-2025年度深度综合研究报告





## 执行摘要



随着大型语言模型（LLM）在生成式人工智能领域的广泛部署，如何从已训练的模型中精准剔除特定数据、知识或行为——即“机器遗忘”（Machine Unlearning）——已成为人工智能安全、隐私合规及版权保护领域的核心挑战。本报告旨在对2023年至2025年间该领域的关键技术进展进行详尽的梳理与深度分析，重点聚焦于四位在该领域具有开创性贡献的学者：**Sijia Liu（刘斯佳）**、**Jamie Hayes**、**Nicolas Papernot** 以及 **Weijia Shi（史伟家）**。

本报告长达约两万字，将深入剖析这四位研究者分别代表的四种截然不同却又互为补充的研究范式：

1. **优化与稀疏性范式（Sijia Liu）**：侧重于通过模型权重的归因分析、二阶优化（SOUL）及稀疏化策略（WAGLE），在数学优化层面解决遗忘效率与模型效用的平衡问题。
2. **生成动力学与政策范式（Jamie Hayes）**：通过教师-学生框架（SCRUB）探索遗忘的动态过程，并从宏观政策角度批判性地揭示了技术能力与监管期望（如“删除”与“抑制”）之间的本质错位。
3. **安全审计与理论界限范式（Nicolas Papernot）**：坚持严格的隐私定义，利用“金丝雀”（Canaries）测试确立隐私泄露的下界，并从差分隐私角度质疑近似遗忘的有效性，强调审计的不可或缺性。
4. **检测与评估范式（Weijia Shi）**：构建了多维度的评估基准（MUSE）和基于概率统计的检测机制（Min-K% Prob），为验证“模型是否真正遗忘”提供了实证工具。

本报告通过对相关文献、预印本、代码库及会议论文的详尽考证，旨在为人工智能研究人员、政策制定者及安全工程师提供一份关于LLM机器遗忘现状的权威参考。

------



## 1. 引言：生成式人工智能中的选择性失忆危机





### 1.1 从判别式模型到生成式模型的范式转移



在深度学习的早期阶段，机器遗忘主要应用于图像分类等判别式任务中。其目标相对单一：通过调整决策边界，使模型对特定样本的预测不再依赖于该样本的特征。然而，随着以Transformer架构为核心的大型语言模型（LLM）的兴起，遗忘的定义发生了质的飞跃。LLM不仅仅是分类器，更是知识的压缩体。数据并非存储于单一的神经元中，而是以全息的形式分布于数十亿甚至数万亿的参数之间。

在这种背景下，仅仅让模型“不输出”某段文本（如《哈利·波特》的段落或用户的信用卡号）并不等同于模型“遗忘”了该信息。潜在的知识表征可能依然存在，并可以通过提示工程（Prompt Engineering）或微调攻击（Fine-tuning Attack）被重新激活。因此，LLM的机器遗忘面临着**“删除（Deletion）”与“抑制（Suppression）”的本体论混淆**。



### 1.2 监管压力与技术瓶颈的博弈



近年来，全球数据隐私法规的收紧——特别是欧盟《通用数据保护条例》（GDPR）中的“被遗忘权”（Right to be Forgotten）——为AI开发商带来了巨大的合规压力。然而，对于拥有700亿参数（如Llama-2-70B）的模型而言，针对每一个删除请求都进行从头训练（Retraining from Scratch）在计算成本上是不切实际的。据估算，训练Llama-2-70B耗费了约170万个A100 GPU小时1。

这就催生了对**近似遗忘（Approximate Unlearning）**技术的迫切需求。该技术旨在通过有限的梯度更新，使模型参数逼近“从未见过该数据”的状态。然而，这种近似处理往往伴随着巨大的风险：

- **灾难性遗忘（Catastrophic Forgetting）**：在试图删除特定信息时，模型可能丧失通用的语言推理能力，导致“模型坍塌”。
- **隐私残留（Privacy Residuals）**：即使模型表面上不再生成敏感数据，攻击者仍可能通过成员推理攻击（Membership Inference Attack, MIA）探测出该数据曾存在于训练集中。

本报告分析的四位学者，正是在这一复杂的三角约束（效率、效用、隐私）中探索解决方案的核心人物。

------



## 2. Sijia Liu：算法优化的手术刀——从稀疏性到二阶优化



Sijia Liu（刘斯佳）及其领导的OPTML团队（Optimization for Machine Learning）代表了机器遗忘研究中的**算法优化派**。他们的核心理念是：机器遗忘本质上是一个受约束的优化问题，通过深入理解模型权重的几何结构和损失曲面的曲率，可以实现对外科手术般精准的“记忆切除”。



### 2.1 “先剪枝，后遗忘”范式（Prune First, Then Unlearn）



在2023年的NeurIPS会议上，Liu团队提出了一项反直觉的研究成果：**模型稀疏性可以简化机器遗忘（Model Sparsity Can Simplify Machine Unlearning）** 2。这一发现挑战了“稠密模型更易优化”的传统认知。



#### 2.1.1 稀疏性作为正则化先验



传统的遗忘方法通常直接在全参数空间上进行梯度上升（Gradient Ascent）。然而，全参数空间的维度极高，且包含了大量对特定知识贡献甚微的冗余权重。Liu的研究表明，如果在遗忘过程之前引入**稀疏先验（Sparse Prior）**，即通过权重剪枝（Weight Pruning）将模型稀疏化，可以显著改变优化轨迹。

- **机制解析**：稀疏化实际上是将优化问题限制在一个低维子空间内。在这个子空间中，遗忘特定样本所需的参数扰动更小，且不太容易波及到其他非目标知识的表征。
- **效率提升**：稀疏模型在计算梯度时涉及的参数更少，从而加速了遗忘过程。更重要的是，稀疏性本身充当了一种正则化项，防止了模型在为了“遗忘”而剧烈调整权重时发生过拟合（即导致模型在保留数据集上的性能崩溃）。
- **实证结果**：研究显示，在结合了稀疏感知（Sparsity-aware）的正则化策略后，微调（Fine-tuning）作为一种简单的近似遗忘手段，其遗忘效能（Unlearning Efficacy）提升了惊人的**77%**。这一数据强有力地证明了模型架构本身的特性（如稀疏度）是解决遗忘问题的关键变量。



#### 2.1.2 稀疏感知遗忘算法



基于上述理论，Liu团队开发了一种**稀疏感知遗忘方法**。该方法并不直接对原始模型进行操作，而是遵循“先剪枝，后遗忘”的操作流：

1. **剪枝阶段**：利用幅度剪枝或结构化剪枝技术，识别并保留对模型通用能力至关重要的权重，剔除冗余权重。
2. **遗忘阶段**：在稀疏掩码（Mask）的约束下，仅对非零权重进行针对遗忘目标的梯度更新。

这一范式的提出，标志着遗忘技术从纯粹的“数据中心”视角（即仅关注如何构建遗忘数据集）向“模型中心”视角（即关注模型结构对遗忘的影响）的转变。



### 2.2 SOUL：二阶优化解锁遗忘潜力



针对一阶优化方法（如随机梯度下降/上升）在处理LLM复杂非凸损失曲面时的局限性，Liu团队在2024年提出了**SOUL（Second-Order UnLearning）**框架 3。



#### 2.2.1 一阶方法的局限性：盲目地修正



目前主流的遗忘算法多采用梯度上升（Gradient Ascent, GA）或梯度差分（Gradient Difference）。这些一阶方法仅利用梯度的方向信息，试图将模型推离遗忘数据的极小值点。然而，LLM的损失景观（Loss Landscape）充满了陡峭的悬崖和狭窄的山谷。一阶方法缺乏对曲率（Curvature）的感知，容易导致步长过大而破坏模型的通用能力，或者步长过小而无法彻底遗忘。



#### 2.2.2 SOUL的核心机制：引入海森矩阵



SOUL框架的核心创新在于将经典的**影响函数（Influence Functions）**理论与现代的二阶优化器相结合。

- **理论重构**：传统的影响函数通常用于一次性估计删除某个样本对参数的影响（One-shot Update）。但在LLM这种高度非线性的模型中，一次性估计的误差极大。SOUL将影响函数的思想重构为一个**动态迭代优化过程**。
- **Sophia优化器的适配**：SOUL基于**Sophia（Second-order Clipped Stochastic Optimization）**优化器构建。Sophia利用海森矩阵（Hessian Matrix）的对角线近似来估计曲率，从而自适应地调整每个参数的更新步长。
- **WoodFisher近似**：为了解决计算海森矩阵逆矩阵（Hessian Inversion）的高昂成本（$O(N^3)$），SOUL采用了WoodFisher等近似技术，使得在数十亿参数的LLM上应用二阶信息成为可能。



#### 2.2.3 实验验证：TOFU基准



在**TOFU（Task of Fictitious Unlearning）**基准测试中，SOUL展现了压倒性的优势。

- **定性分析**：相比于梯度差分（GradDiff）和偏好优化（PO），SOUL处理后的模型在回答非遗忘问题时（Utility），生成的文本更加流畅、逻辑连贯，没有出现常见的“胡言乱语”现象。
- **定量数据**：在衡量遗忘质量（Forget Quality）与模型效用（Model Utility）的综合指标上，SOUL始终优于一阶基线方法。这表明，利用二阶信息可以找到一条更“平滑”的路径撤出特定记忆，而不至于震荡整个参数大厦。



### 2.3 WAGLE：策略性权重归因与模块化遗忘



为了进一步解决“不知道该改哪些参数”的黑盒问题，Liu团队在2024年末推出了**WAGLE（Weight Attribution-Guided LLM Unlearning）** 7。



#### 2.3.1 权重归因的必要性



在LLM中，并非所有参数都承载着相同的知识。盲目更新所有参数不仅效率低下，而且增加了破坏无关知识的风险。WAGLE旨在回答一个核心问题：**哪些特定的权重对生成目标遗忘内容负有主要责任？**



#### 2.3.2 双层优化（Bi-Level Optimization, BLO）视角



WAGLE不仅是一个算法，更是一个通用的框架。它将权重归因问题形式化为一个双层优化问题：

- **内层循环（Inner Loop）**：模拟在特定权重子集上进行遗忘操作后的模型状态。
- **外层循环（Outer Loop）**：优化一个二进制或连续的掩码（Mask），该掩码决定了哪些权重应该参与更新，以最大化遗忘损失（Forget Loss）同时最小化保留损失（Retain Loss）。



#### 2.3.3 数学表达与闭式解



WAGLE推导出了评估权重影响力的闭式解（Closed-form Solution）。公式涉及对遗忘损失梯度 $\nabla \ell_f$ 和保留损失梯度 $\nabla \ell_r$ 的相互作用项分析：



$$S_i \approx \mu (\theta_i - \nabla \ell_r / \gamma) \odot \nabla \ell_f$$



该公式量化了每个参数 $\theta_i$ 在平衡遗忘与保留任务中的贡献度。



#### 2.3.4 模块化与通用性



WAGLE的一个显著特点是其**模块化（Modularity）**。它不依赖于特定的损失函数，可以无缝集成到现有的遗忘算法中，如梯度差分（GradDiff）、负偏好优化（NPO）等。实验表明，在集成WAGLE生成的权重掩码后，现有SOTA方法的性能均得到了显著提升，实现了“外科手术式”的知识切除。



### 2.4 小结：Sijia Liu的工程哲学



Sijia Liu的研究路径清晰地展示了从“蛮力优化”向“精细化控制”的演进。从利用稀疏性作为先验，到利用二阶信息导航损失曲面，再到利用归因分析锁定关键参数，他的工作极大地提升了LLM遗忘的工程可行性与理论深度，为处理大规模模型的遗忘问题提供了坚实的算法基础。

------



## 3. Jamie Hayes：政策与实践的断裂——生成式遗忘的局限性



与Sijia Liu聚焦于算法内部机制不同，来自Google DeepMind的Jamie Hayes更多地从**系统动力学**和**宏观政策**的双重视角审视机器遗忘。他的研究不仅提供了强有力的算法工具（SCRUB），更对整个领域的目标设定提出了深刻的质疑。



### 3.1 SCRUB：无界遗忘与教师-学生框架



在NeurIPS 2023上，Hayes等人提出了**SCRUB（SCalable Remembering and Unlearning unBound）**算法 11。这项工作针对的是此前遗忘方法在扩展性（Scalability）和应用场景通用性上的不足。



#### 3.1.1 教师-学生（Teacher-Student）动力学



SCRUB的核心创新在于引入了一个“全知”的教师模型来指导遗忘过程。

- **教师模型（Teacher, $w^o$）**：即原始的、未经修改的已训练模型。尽管它包含了我们想要遗忘的数据，但它也包含了完美的语言能力和非敏感知识。
- **学生模型（Student, $w^u$）**：从教师模型初始化，是实际执行遗忘操作的对象。

SCRUB构建了一个混合蒸馏（Hybrid Distillation）目标函数：

1. 服从（Obey）：在保留数据集（$D_{\text{retain}}$）上，学生模型必须尽可能模仿教师模型的输出分布。这是通过最小化两者输出概率分布的KL散度（KL-Divergence）来实现的：

   $$ \min D_{KL}(p(f(x; w^o)) || p(f(x; w^u))), \quad x \in D_{\text{retain}} $$

这确保了模型的通用能力不会因为遗忘操作而退化。

2. 反叛（Disobey）：在遗忘数据集（$D_{\text{forget}}$）上，学生模型被强迫去最大化与教师模型预测的偏差，或者最大化该数据的预测误差。



#### 3.1.2 “回溯”（Rewinding）机制与成员推理防御



Hayes敏锐地指出了一个常见的陷阱：如果模型在遗忘集上的表现**过差**（例如，错误率100%），这本身就是一种信息泄露。因为一个从未见过该数据的模型（Retrained-from-scratch）通常会有正常的错误率，而不是极端的错误率。这种“负向记忆”很容易被成员推理攻击（MIA）利用。

为了解决这个问题，SCRUB引入了**回溯机制（Rewinding）**：

- **验证集对齐**：算法在训练过程中持续监控模型在留出验证集（Holdout Set）上的损失。
- **最佳检查点选择**：它不是选择遗忘损失最大的点，而是回溯到那个遗忘集上的误差**“刚刚好高”（Just high enough）**的点——即与验证集误差相近的点。这使得遗忘后的模型在行为上真正难以与未训练模型区分。



#### 3.1.3 “无界”（Unbounded）的含义



SCRUB被称为“无界”，是因为它不再受限于为了获得理论证明（如凸性假设）而牺牲模型复杂度的传统框架。它是一个实用的、适用于深度神经网络的通用框架，能够同时处理消除偏差（Removing Biases）、解决混淆（Resolving Confusion）和用户隐私（User Privacy）等多种任务。



### 3.2 政策与技术的错位：2025年的深刻反思



在NeurIPS 2025发表的重磅论文《机器遗忘并未如你所想：生成式AI政策与研究的教训》（Machine Unlearning Doesn't Do What You Think）中，Hayes联合Papernot及多位法学专家，对当前遗忘研究的轨道进行了系统性批判 14。



#### 3.2.1 五大根本性错位（Five Fundamental Mismatches）



Hayes指出，技术界的“遗忘”与政策制定者（如GDPR起草者）眼中的“遗忘”存在巨大的语义鸿沟：

| **错位编号**   | **核心观点**                 | **详细解析**                                                 |
| -------------- | ---------------------------- | ------------------------------------------------------------ |
| **Mismatch 1** | **输出抑制 $\neq$ 数据移除** | 仅仅让模型不输出某些内容（Output Suppression），并不代表该数据已从参数中移除。这就像是给模型戴上了口套，而不是切除了记忆。 |
| **Mismatch 2** | **数据移除 $\neq$ 输出抑制** | 即使通过精确的遗忘算法移除了特定样本（Data Removal），模型仍可能通过泛化能力或冗余数据生成相似的内容。例如，删除了所有关于“蜘蛛侠”的训练数据，模型仍可能根据“红蓝紧身衣超级英雄”生成蜘蛛侠形象。 |
| **Mismatch 3** | **模型 $\neq$ 数据库**       | 政策制定者习惯于数据库思维（删除一行记录），但神经网络是全息的。信息并不是存储在特定的“抽屉”里，而是弥散在整个网络中。 |
| **Mismatch 4** | **模型不等同于其输出**       | 模型的内部状态（权重、激活值）可能包含大量未在当前输出中显现的潜在知识。 |
| **Mismatch 5** | **意外后果**                 | 强制遗忘可能导致模型产生幻觉或在其他安全领域失效（如删除了仇恨言论数据后，模型可能反而无法识别隐晦的仇恨言论）。 |



#### 3.2.2 法律与伦理含义



Hayes强调，如果目标是**版权保护**，那么“输出抑制”（让模型吐不出侵权内容）可能就足够了。但如果目标是**隐私保护**或**被遗忘权**，目前的近似遗忘技术大多属于“输出抑制”，无法确何潜在隐私不被提取（例如通过越狱攻击）。因此，将当前的遗忘技术作为满足隐私法规的灵丹妙药是极度危险的。

------



## 4. Nicolas Papernot：安全审计的守门人——从差分隐私到金丝雀



Nicolas Papernot（多伦多大学）在机器遗忘领域扮演着“审计师”与“安全守门人”的角色。他的研究以严谨著称，致力于通过对抗性攻击和理论证明来戳破近似遗忘的虚假安全感。



### 4.1 近似遗忘的理论困境



在2021-2022年的一系列基础性工作中，Papernot首先从理论上解构了“近似遗忘”定义的逻辑漏洞 16。



#### 4.1.1 参数非唯一性证明



他指出，在深度学习中，从数据集到模型参数的映射并不是双射（Bijective）的。换句话说，两个完全不同的数据集（一个包含敏感数据，一个不包含）可能训练出参数完全相同的模型。

- **推论**：仅凭检查模型参数（权重），在数学上**不可能证明**某个特定数据点$x$未被用于训练。
- **算法定义的必要性**：因此，遗忘不能定义为达到某种参数状态，而必须定义为执行了某种**可审计的算法过程**。除了SISA（Sharded, Isolated, Sliced, Aggregated）这种物理隔离数据并重新训练特定分片的方法外，其他所谓“修改权重”的方法都缺乏严格的理论根基。



### 4.2 隐私审计：利用“金丝雀”测量遗忘



鉴于无法直接查看模型“灵魂”，Papernot在2025年的论文《大型语言模型的隐私审计》（Privacy Auditing of Large Language Models）中，提出了一种基于**金丝雀（Canaries）**的实证审计方法 18。



#### 4.2.1 金丝雀设计方法论



传统的审计方法往往低估了模型的记忆能力。Papernot设计了一种更强的审计工具：

- **结构**：金丝雀样本由两部分组成：一个独特的**前缀（Prefix）和一个高熵的随机秘密（Secret）**。
  - 例如：“操作代号蓝鸟的启动密码是 [随机字符串X8z9...]”。
- **注入与测试**：将这些金丝雀混入训练数据。在遗忘操作后，通过向模型输入前缀，观察模型是否能补全那个随机的秘密。如果能，说明遗忘失败。



#### 4.2.2 惊人的实验结果：$\epsilon \approx 1$



Papernot的研究改进了金丝雀的设计，使其更易被模型记忆，从而提高了审计的灵敏度。

- 在**Qwen2.5-0.5B**模型上的实验显示，他设计的金丝雀检测达到了**49.6%的真阳性率（TPR）**，而误报率（FPR）仅为1%。相比之下，先前方法的TPR仅为4.2%。
- 这一结果使得研究人员首次能够在黑盒设置下，为LLM训练推导出$\epsilon \approx 1$的差分隐私下界。这意味着现有的LLM对训练数据的记忆程度远超预期，且现有的简单遗忘方法远未能达到安全标准。



### 4.3 成员推理攻击（MIA）作为黄金标准



Papernot坚定地主张，**成员推理攻击（Membership Inference Attack, MIA）**不仅是攻击手段，更是衡量遗忘成功与否的黄金标准。

- 如果攻击者能以高于随机猜测的概率判断出某个样本是否在训练集中，那么该样本就没有被真正遗忘。
- 他批评了许多仅使用“遗忘集准确率下降”作为指标的研究。他认为，模型可能在准确率上表现为“不会做这道题”，但在概率分布上仍保留了该数据的统计特征（即Min-K% Prob依然异常）。



### 4.4 差分隐私（DP）与遗忘的辩证关系



Papernot认为，**差分隐私（Differential Privacy）**是目前唯一能提供形式化遗忘保证的框架 21。

- 如果在训练阶段就使用了DP-SGD（$\epsilon$有限），那么任何单一特定样本对模型的影响本身就是受限的。此时，遗忘可能仅仅是“免费”的或者是容易实现的。
- 对于非DP训练的模型，事后的“近似遗忘”操作在最坏情况下只能被视为一种混淆（Obfuscation），而非真正的清除。

------



## 5. Weijia Shi：度量的艺术——基准测试与探测机制



如果说Liu在造发动机，Hayes在制定交通规则，Papernot在做碰撞测试，那么Weijia Shi（史伟家，华盛顿大学）则是在设计精密的仪表盘和排放检测系统。她的工作聚焦于如何客观、量化地评估“遗忘”到底发生了没有。



### 5.1 Min-K% Prob：探测未知的训练数据



在ICLR 2024发表的《从大型语言模型中检测预训练数据》一文中，Shi提出了一种革命性的检测方法：**Min-K% Prob** 23。



#### 5.1.1 核心假设：离群词理论



传统的检测方法（如困惑度Perplexity）往往失效，因为LLM对所有流畅的文本都会给出高概率。Shi提出了一个基于统计规律的假设：

- 对于模型**未见过**的文本：虽然整体通顺，但其中必然包含一些对于模型而言概率较低的“离群词”（Outlier words）。
- 对于模型**见过**（训练过）的文本：模型会过度拟合这些数据，导致即使是原本生僻、概率低的词，其预测概率也会显著升高。



#### 5.1.2 算法流程



Min-K% Prob算法不需要训练额外的参考模型，完全基于目标模型的输出logits：

1. **标记化（Tokenization）**：将输入文本切分为Token序列。
2. **概率计算**：计算每个Token在给定前文条件下的对数似然（Log-likelihood）。
3. **筛选离群值**：选取概率最低的那$k%$个Token。
4. **聚合评分**：计算这$k%$个Token的平均对数似然。
5. **判定**：如果这个平均值异常高（即原本应该低概率的词现在的概率很高），则判定该文本存在于训练集中。



#### 5.1.3 在遗忘中的应用



Min-K% Prob成为了验证机器遗忘效果的“照妖镜”。一个成功的遗忘算法，不仅要让模型无法背诵文本（Verbatim），更必须让遗忘数据的Min-K% Prob分数下降到与留出集（Holdout Set）相同的水平。实验表明，许多声称成功的遗忘方法在Min-K%测试下均原形毕露。



### 5.2 MUSE：六维评估基准（Six-Way Evaluation）



针对领域内评估指标碎片化、甚至“挑选指标”（Cherry-picking）的乱象，Shi主导开发了**MUSE（Machine Unlearning Six-Way Evaluation）**基准 26。



#### **5.2.1 六大维度（The Six Dimensions）**



**MUSE首次系统性地将数据所有者（Data Owner）的需求与模型部署者（Model Deployer）的需求对立统一起来：**

**数据所有者视角（关注彻底性）：**

1. **逐字记忆消除（No Verbatim Memorization）：使用ROUGE-L指标，测试模型能否补全遗忘文本的后缀。**
2. **知识记忆消除（No Knowledge Memorization）：测试模型能否回答关于遗忘内容的知识性问题（QA Accuracy）。**
3. **隐私泄露消除（No Privacy Leakage）：利用Min-K% Prob等MIA工具，检测数据留存的统计痕迹。**

**模型部署者视角（关注可用性）：**

4. **效用保留（Utility Preservation）：在通用任务（如MMLU、新闻摘要）上的性能是否下降。**

5. **可扩展性（Scalability）：当遗忘集（Forget Set）变大时（如从单一特定样本扩展到整个版权书库），算法是否依然有效。**

6. **可持续性（Sustainability）：在连续处理一系列遗忘请求后，模型是否会逐渐崩溃（Model Collapse）。**



#### 5.2.2 基准测试发现



利用MUSE对Llama-2-7b等模型进行的广泛测试揭示了残酷的现状：**目前的遗忘算法难以同时满足这六个维度。**

- 能够有效消除逐字记忆的方法（如梯度上升），往往会导致严重的隐私泄露（Min-K%分数居高不下）或效用受损。
- 能够保护效用的方法，往往遗忘得不彻底，特别是在面对知识性提问时容易“露馅”。
- 可持续性是一个巨大的短板，连续遗忘往往导致模型语言能力的快速退化。

------



## 6. 综合分析：隐私、效用与效率的三难困境



通过对上述四位学者工作的交叉比对，我们可以清晰地看到当前LLM机器遗忘领域面临的核心矛盾。



### 6.1 知识与Token的二元对立



- **Sijia Liu的WAGLE** 试图通过定位编码“知识”的权重来解决问题，倾向于认为知识是存储在特定参数中的。
- **Weijia Shi的Min-K%** 揭示了即便“概念”似乎消失了，**Token层面的统计痕迹**依然顽固存在。这意味着模型可能不再知道“哈利·波特是谁”，但仍然赋予“哈利”后面接“波特”极高的概率。
- **Jamie Hayes** 指出，删除Token痕迹并不等同于删除了概念。这种**语义（Semantics）与统计（Statistics）的脱节**是当前最大的技术挑战。



### 6.2 乐观主义与悲观主义的碰撞



- **算法乐观派（Liu）**：认为通过更精密的数学工具（稀疏性、二阶优化、双层规划），我们最终可以逼近“精确遗忘”的效果，实现工程上的可用解。
- **理论悲观派（Papernot）**：坚持认为在缺乏差分隐私训练的前提下，任何事后的遗忘操作都是徒劳的（Futile）。他证明了权重的不可审计性，实际上宣判了当前所有启发式算法在严格隐私定义下的死刑。



### 6.3 评估标准的演进



领域内的评估标准正在经历从定性到定量的深刻变革：

- **第一阶段（2023前）**：简单的准确率下降（Forget Accuracy）。
- **第二阶段（Hayes）**：引入“回溯机制”，要求遗忘后的行为分布与留出集一致，而非单纯追求高误差。
- **第三阶段（Shi & Papernot）**：引入**对抗性审计**（Canaries）和**统计探测**（Min-K%），将评估战场转移到了模型概率分布的深层结构上。

------



## 7. 结论与展望



2023年至2025年的研究表明，LLM机器遗忘已经从一个边缘的学术兴趣演变为关乎AI安全与合规的各种核心议题。

1. **算法层面**：简单的梯度上升已成过去式。**Sijia Liu**的工作证明，未来的遗忘算法必须是**二阶的（SOUL）**、**结构感知的（Sparsity）和精细归因的（WAGLE）**。这不仅是提升效果的需要，更是保护模型通用能力的唯一途径。
2. **政策层面**：**Jamie Hayes**的警告必须引起重视。政策制定者需要理解“删除数据”在AI语境下的物理不可能性。法律术语需要从“删除”转向对“输出抑制”和“隐私审计”的精细界定。
3. **验证层面**：**Nicolas Papernot**和**Weijia Shi**构建的审计大坝将变得越来越高。任何声称符合GDPR要求的商业模型，未来都必须通过Min-K%测试和金丝雀审计。

未来展望：

未来的研究方向可能不再是寻找单一的“遗忘算法”，而是构建一个全生命周期的隐私管理系统。这可能包括：在预训练阶段引入轻量级的差分隐私（Papernot的愿景）；在模型架构中预留便于遗忘的稀疏模块（Liu的WAGLE与Sparsity）；以及在部署端实施动态的、基于教师-学生框架的实时抑制（Hayes的SCRUB）。只有通过这种全栈式的融合，我们才能在生成式AI时代真正实现对数据的掌控。

------



### 数据图表：关键方法特性对比



| **维度**         | **WAGLE / SOUL (Sijia Liu)**             | **SCRUB (Jamie Hayes)**                  | **SISA / Canaries (Nicolas Papernot)**       | **Min-K% / MUSE (Weijia Shi)**   |
| ---------------- | ---------------------------------------- | ---------------------------------------- | -------------------------------------------- | -------------------------------- |
| **核心目标**     | 优化效率、计算可行性、模型效用保护       | 解决策略与动态、无界扩展性               | 理论安全性、可审计性、差分隐私               | 经验性测量、多维度基准测试       |
| **技术手段**     | 二阶优化（Hessian）、权重掩码、稀疏先验  | 教师-学生蒸馏、回溯机制（Rewinding）     | 数据分片重训、金丝雀注入、MIA                | 概率离群值分析、六维评估体系     |
| **对权重的视角** | 权重是稀疏的、可归因的、具有几何结构的   | 权重是灵活的，需由教师模型引导           | 权重是非唯一的（Non-unique），不可信的       | 权重是黑盒，仅关注输出概率分布   |
| **关键洞察**     | **结构化稀疏**能缩小近似与精确遗忘的差距 | 最大化误差是危险的，需**对齐留出集分布** | 近似遗忘在**理论上无法证明**，需依赖算法过程 | 未见过的文本包含**低概率离群词** |



### 引用索引



- 2 Model Sparsity Can Simplify Machine Unlearning (NeurIPS 2023)
- 30 Rethinking Machine Unlearning for Large Language Models (Nature MI 2025)
- 14 Machine Unlearning Doesn't Do What You Think (NeurIPS 2025)
- 18 Privacy Auditing of Large Language Models (2025)
- 1 LLM Unlearning challenges and efficiency
- 26 MUSE: Machine Unlearning Six-Way Evaluation
- 7 WAGLE: Strategic Weight Attribution
- 3 SOUL: Second-Order UnLearning
- 11 Towards Unbounded Machine Unlearning (SCRUB)
- 23 Detecting Pretraining Data (Min-K% Prob)
- 16 On the Necessity of Auditable Algorithmic Definitions
- 21 Differential Privacy and Unlearning definitions