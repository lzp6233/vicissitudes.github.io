# model edit and unlearning

## unlearn method

### 基于优化的微调

+ GA
  + +Retain
  + +KL

+ **NPO 负向偏好优化**
+ Knowledge Negation / I don't know 

### 基于表示工程的方法

+ **RMU 表示误导**

### 其他

**Localization-informed**: 先定位对特定知识重要的神经元或权重，然后进行编辑或修剪 。

**Input-based (In-context)**: 不修改模型参数，通过提示词（Prompting）或上下文示例来引导模型忽略特定信息 。

**Influence Function-based**: 利用影响函数估算训练数据对模型参数的影响并进行逆向操作 。

### 评估指标

+ Forget Quality / Efficacy
  + **Forget Accuracy / Perplexity**: 模型在遗忘集上的问答准确率或困惑度。理想情况下应显著下降（或上升至无知水平） 。
  + **Forget Distance (KL Divergence)**: 遗忘后模型与“重训练模型”（Retrained Model，即从未见过该数据的模型）在遗忘集上的输出分布的 KL 散度。该值越小越好，代表真正逼近了重训练的效果 。
  + **Forget Quality (KS Test)**: 在 TOFU 基准中，通过 Kolmogorov-Smirnov (KS) 检验来比较“遗忘后模型”与“重训练模型”在遗忘集上的 Truth Ratio 分布。p-value > 0.05 表示两者不可区分，即遗忘成功 。
  + **MaxLogit Distribution**: 检查模型生成遗忘内容时的 Logit 值分布。RMU 研究发现，有效遗忘后，模型生成的 token 应该具有较低的置信度（较低的 MaxLogit 值） 
+ Model Utility / Retain Quality
  + **Retain Accuracy**: 模型在保留集（Retain Set）或通用基准（如 MMLU, ARC, TruthfulQA）上的准确率 。
  + **Retain Distance**: 遗忘后模型与初始模型（或重训练模型）在保留集上的 KL 散度 。
  + **Fluency / Readability**: 检查模型输出是否流畅，避免出现 GA 方法常见的乱码（Gibberish）现象 
+ Safety & Privacy Audit
  + MIA
  + SPV-MIA
  + Jailbreak Robutness
  + Relearning Attack

