# 2024-2025年大语言模型机器遗忘（LLM Unlearning）前沿技术深度调研报告





## 1. 引言：机器遗忘的范式转变与核心挑战



随着大语言模型（LLM）在参数规模和训练数据量上的指数级增长，其在各个领域的应用日益广泛。然而，这种增长也带来了严峻的数据治理挑战。模型在预训练和微调阶段不可避免地记忆了训练语料中的敏感个人信息（PII）、受版权保护的内容以及有害知识（如生物武器制造、网络攻击代码等）1。在《通用数据保护条例》（GDPR）确立“被遗忘权”以及全球范围内对AI安全监管日益收紧的背景下，如何从已训练好的模型中精准剔除特定知识，同时不损害模型的通用能力，成为了AI安全与对齐领域的各种核心议题 4。

传统的解决方案是剔除数据后重新训练（Retraining from Scratch），但这对于动辄数千亿参数的LLM而言，其计算成本和时间成本是不可接受的。因此，**大语言模型遗忘（LLM Unlearning）**应运而生。这是一种试图通过高效的后训练（Post-training）手段，在参数空间或表示空间进行微创手术，以达到与重新训练模型在行为上不可区分的状态 6。

本报告基于2024年至2025年初的最新学术文献，对当前LLM遗忘技术进行了详尽的调研与分析。报告将深入探讨基于优化的方法（如NPO、SimNPO）、基于表示工程的方法（如RMU、Attention Shifting）、参数高效的持续遗忘框架（如O3）以及向量算术方法（如Task Vectors）。同时，我们将结合TOFU、MUSE、WMDP等最新基准测试，评估这些方法的实际效果、副作用（如幻觉、灾难性遗忘）以及面临的深层挑战（如遗忘痕迹残留、重学习攻击）。



## 2. 理论框架与问题定义



在深入具体算法之前，必须建立LLM遗忘的数学与理论框架。



### 2.1 问题形式化



给定一个预训练模型 $M_{\theta}$ 和其训练数据集 $D$，我们将数据集划分为遗忘集（Forget Set, $D_f$）和保留集（Retain Set, $D_r$），即 $D = D_f \cup D_r$。

机器遗忘的目标是获得一个新的模型参数 $\theta^*$，使得：

1. **遗忘有效性（Unlearning Efficacy）：** 模型在 $D_f$ 上的表现与其从未在 $D_f$ 上训练过的模型（Retrained Model）一致。这通常通过测量模型在 $D_f$ 上的准确率、困惑度（Perplexity）或通过成员推理攻击（Membership Inference Attacks, MIA）来验证 4。
2. **效用保留（Utility Preservation）：** 模型在 $D_r$ 上的表现应尽可能接近原始模型 $M_{\theta}$。这要求模型不发生灾难性遗忘（Catastrophic Forgetting），保持其语言生成能力和逻辑推理能力 6。
3. **效率（Efficiency）：** 获得 $\theta^*$ 的计算成本应远低于在 $D_r$ 上重新训练的成本 7。



### 2.2 遗忘的分类学



根据最新的综述与研究，当前的遗忘方法可以从机制上分为三大类：

- **基于优化的方法（Optimization-based）：** 通过设计特定的损失函数，利用梯度下降（或上升）直接更新模型权重。
- **表示工程与导向（Representation Engineering & Steering）：** 不直接擦除知识，而是通过操纵中间层的激活模式，阻断知识的提取路径。
- **模型算术与合并（Model Arithmetic & Merging）：** 基于参数空间的线性假设，通过向量加减法来抵消特定任务的权重。

------



## 3. 基于优化的遗忘方法：从梯度上升到偏好学习



这一类方法的核心思想是将遗忘过程视为“负向训练”或“对齐”问题。



### 3.1 梯度上升（Gradient Ascent, GA）及其局限性



最直观的方法是在遗忘集 $D_f$ 上最大化交叉熵损失（Cross-Entropy Loss），即梯度上升。



$$\theta_{new} = \theta_{old} + \alpha \nabla_{\theta} \mathcal{L}_{CE}(D_f)$$



然而，2024年的多项研究（特别是基于TOFU基准的评估）表明，GA方法极不稳定 8。

- **灾难性崩溃（Catastrophic Collapse）：** GA往往会导致模型权重发散，使得模型在生成遗忘内容时输出乱码，甚至破坏模型的语言模型头（LM Head），导致其在保留集上的能力大幅下降。
- **不精确的遗忘：** GA只是单纯地破坏预测概率，并没有真正将参数导向“未学习”的状态，而是导向了“损坏”的状态 10。



### 3.2 负向偏好优化（Negative Preference Optimization, NPO）



为了解决GA的崩溃问题，Zhang等人（2024）提出了**NPO**。该方法受直接偏好优化（DPO）的启发，将遗忘集视为“被拒绝”（Rejected）的样本，而将参考模型（Reference Model，即遗忘前的原始模型）作为锚点 11。

原理与机制：

NPO 的损失函数设计旨在降低遗忘样本生成概率的同时，限制模型参数不偏离原始模型太远（通过KL散度约束隐含在损失函数中）。

$$ \mathcal{L}{NPO} = - \log \sigma \left( \beta \log \frac{\pi\theta(y|x)}{\pi_{ref}(y|x)} - \text{margin} \right) $$

其中 $\pi_{ref}$ 是原始模型，$\beta$ 是控制偏好强度的超参数。

**解决的问题与效果：**

- **稳定性提升：** 实验显示，NPO在TOFU基准上表现出比GA更好的稳定性，能够在有效降低遗忘集准确率的同时，维持较高的模型通用效用（Model Utility）9。
- **收敛特性：** 理论分析证明，NPO在最小化损失的过程中，导致模型崩溃的速度呈指数级低于GA 11。
- **局限性：** NPO 依然依赖于参考模型 $\pi_{ref}$。研究指出，这种依赖引入了“参考模型偏差”（Reference Model Bias）。对于参考模型本身记忆非常牢固的“困难样本”，NPO的梯度信号可能不足，导致遗忘不彻底；而对于简单样本，则可能导致过度遗忘 12。



### 3.3 SimNPO：去参考模型的简单负向偏好优化



针对NPO的参考模型偏差问题，Fan等人（2025）提出了**SimNPO** 12。

原理与机制：

SimNPO 摒弃了在训练过程中实时调用参考模型 $\pi_{ref}$ 的做法，而是采用了一种长度归一化（Length-Normalized）的无参考目标函数。

其核心洞察在于，参考模型的存在不仅增加了计算开销（需要加载两个模型），而且会使得优化过程被参考模型的先验分布所“锚定”。SimNPO 直接优化模型以降低遗忘样本的似然值，但引入了长度归一化项来消除生成长度对梯度的影响，从而实现更均衡的优化。

**效果对比：**

- **更高效的遗忘：** 在MUSE和TOFU基准上的测试表明，SimNPO在遗忘质量（Forget Quality）上优于NPO，特别是在处理不同难度的遗忘数据时表现出更好的一致性 12。
- **计算效率：** 由于不需要前向传播参考模型，SimNPO的显存占用和训练时间均有所减少。
- **通用性：** 即使在 $\beta \to 0$ 的极限情况下（退化为加权梯度下降），SimNPO 的表现依然优于传统的梯度差异（GradDiff）方法 14。



### 3.4 梯度差异（Gradient Difference, GradDiff）与KL正则化



另一类优化方法试图通过显式地平衡遗忘与保留。GradDiff 计算遗忘集的梯度（上升方向）和保留集的梯度（下降方向），并将两者相加更新参数 16。



$$\nabla \mathcal{L}_{total} = \nabla \mathcal{L}_{retain} - \lambda \nabla \mathcal{L}_{forget}$$



然而，实际操作中 $\lambda$ 的调节极为困难。为了防止模型参数漂移，通常会加入KL散度正则项（KLD），约束模型输出分布与原始模型保持一致 17。虽然这种方法理论完备，但在大规模模型上，计算Hessian矩阵或精确的KL散度开销巨大，通常只能采用近似方法。

------



## 4. 基于表示工程的遗忘方法：操控与导向



与直接修改存储知识的权重不同，表示工程（Representation Engineering）侧重于干扰模型在推理过程中对特定知识的激活与提取。这类方法通常更加轻量级，且能更好地保护无关知识。



### 4.1 表示误导（Representation Misdirection for Unlearning, RMU）



RMU 是该领域的代表性工作之一，由Li等人（2024）提出 18。

原理与机制：

RMU 的核心思想是“神经操控”（Neural Steering）。它并不试图擦除权重中的事实，而是训练模型的一个轻量级适配器（Adapter）或仅微调少量层，使得当输入为遗忘集内容时，模型的中间层激活向量（Activation Vector）被“推”向一个随机向量或预定义的无关向量。

$$ \mathcal{L}_{RMU} = |

| H(x_f) - v_{random} ||^2 + \gamma |

| H(x_r) - H_{orig}(x_r) ||^2 $$

- 第一项强制遗忘样本的激活 $H(x_f)$ 接近随机噪声 $v_{random}$。
- 第二项强制保留样本的激活 $H(x_r)$ 保持不变。

**效果与局限：**

- **优点：** RMU 在保留模型通用能力方面表现出色，因为它几乎不改变模型处理通用语言的机制。在WMDP（大规模杀伤性武器）基准测试中，RMU成功地抑制了模型回答危险问题的能力 3。
- **缺点（遗忘痕迹）：** 后续研究（Dang et al., 2025; Li et al., 2024）发现，RMU 更多起到的是“掩盖”作用而非“擦除”。通过线性探测（Linear Probing）或对模型进行轻微的重学习（Relearning），被RMU屏蔽的知识很容易恢复。这表明知识依然存在于参数中，只是被路由到了错误的输出空间 19。
- **层敏感性：** RMU 的效果对选择哪一层进行操控非常敏感。如果施加在浅层，可能会影响深层的语义理解；施加在深层，则可能无法有效阻断知识流 23。



### **4.2 自适应表示误导（Adaptive RMU）**



**为了解决RMU在不同层和不同样本上的不稳定性，Dang等人（2025）提出了Adaptive RMU 22。**

**原理：**

**传统的RMU使用固定的系数来控制导向随机向量的强度。Adaptive RMU 引入了自适应机制，根据遗忘样本当前激活向量的范数（Norm）动态调整导向系数。**

**$$ \alpha_{adaptive} \propto || H(x_f) || $$**

**这种机制确保了对于激活强烈的“顽固”知识，施加更大的误导力度；而对于激活较弱的知识，则施加较小的力度，从而避免过度遗忘或遗忘不足。**

**效果：**

**实验表明，Adaptive RMU 能够显著提高遗忘的鲁棒性，特别是在对抗性攻击（如越狱攻击）面前，其防御能力强于标准RMU，并且在更多层的选择上具有通用性 24。**



### 4.3 注意力转移（Attention Shifting, AS）



针对遗忘后容易产生**幻觉（Hallucination）\**的问题，Chen等人（2025）提出了\**Attention Shifting** 25。

问题背景：

当使用GA或NPO等方法强行压制模型对正确答案的预测概率时，模型往往会将概率分配给次优的token，导致生成看似通顺但事实错误的幻觉内容。例如，遗忘“哈利波特”后，模型可能会说“哈利波特是《星球大战》中的角色”。

原理与机制：

AS 并不直接攻击输出概率，而是干预注意力机制（Self-Attention）。

- 它训练一个轻量级的Adapter，专门识别遗忘集中的“事实承载Token”（Fact-bearing Tokens）。
- 当检测到这些Token时，Adapter会抑制其Attention Score，并将注意力权重重新分配给句子中的“中性Token”或“功能词”。
- 这种“去关注”（De-attention）机制使得模型在生成过程中无法聚焦于特定实体，从而自然地阻断了知识的提取，而不是强行扭曲输出 25。

效果：

在TOFU和TDEC基准上，AS方法在保持遗忘效果的同时，显著降低了幻觉率（Hallucination Rate），使得模型更倾向于输出“我不知道”或保持沉默，而非胡编乱造，提升了遗忘后的安全性 25。

------



## 5. 向量算术与模型合并：非训练的遗忘



这一类方法基于“任务向量”（Task Vector）的线性假设，即模型的特定能力可以由参数空间中的一个方向向量来表示。



### 5.1 任务向量否定（Task Vector Negation）



Ilharco等人（2023）提出的任务向量方法在2024年得到了广泛验证和扩展 26。

原理：

假设预训练模型为 $\theta_{pre}$，在目标遗忘数据上微调后的模型为 $\theta_{ft}$。那么，任务向量 $\tau$ 定义为：



$$\tau = \theta_{ft} - \theta_{pre}$$



要实现遗忘，只需将该向量从当前模型中减去（或者加上其负向量）：



$$\theta_{unlearned} = \theta_{current} - \lambda \tau$$



其中 $\lambda$ 是控制遗忘强度的超参数。

**优势与局限：**

- **高效性：** 这种方法不需要在基础模型上进行昂贵的梯度更新，仅需简单的算术运算，且支持模块化地遗忘多个任务（通过向量加法）。
- **干扰性：** 简单的向量减法可能会误伤与遗忘任务呈正相关的其他能力。例如，遗忘“有毒言论”向量可能会同时降低模型对“负面情绪”的识别能力，导致效用下降 27。
- **超参数敏感性：** $\lambda$ 的选择至关重要。过大导致模型行为反转（Inverted Behavior），过小则遗忘不彻底。



### 5.2 选择性任务算术（Selective Task Arithmetic, STA）



为了解决向量减法的“误伤”问题，Zhang等人（2024）提出了**STA** 30。

原理：

STA 并不减去整个任务向量，而是引入了**参数重要性（Parameter Importance）**的概念。

- 利用Fisher信息矩阵或一阶泰勒展开，评估任务向量中每个参数对于遗忘任务的重要性。

- 仅对那些重要性高且对保留任务重要性低的参数进行减法操作。

  

  $$\theta_{new} = \theta_{current} - \lambda (\tau \odot M_{mask})$$

  

  其中 $M_{mask}$ 是基于重要性生成的掩码。

效果：

STA 实现了更精细的“手术式”遗忘。实验数据显示，在多任务遗忘场景下，STA 能够有效避免不同任务向量之间的干扰，显著提升了模型在保留任务上的性能 30。

------



## 6. 持续遗忘与参数高效框架：面向真实场景



在实际应用中，遗忘请求往往是源源不断到来的（Continual Unlearning）。如果每来一个请求就对模型进行一次全局更新，很快会导致模型彻底遗忘其原始能力，即“保留集遗忘”。



### 6.1 O3框架（Orthogonal LoRA + OOD Detection）



Gao等人（2025）提出的**O3框架**是解决持续遗忘问题的SOTA方案，特别是针对**无法访问保留数据（No Retain Data Access）**的严格隐私场景 31。

**核心组件：**

1. **正交低秩适配器（Orthogonal LoRA）：**
   - O3 不直接修改模型权重，而是为每个遗忘请求训练一个LoRA模块。
   - 关键创新在于引入了**正交性约束（Orthogonal Constraint）**。它强制新训练的LoRA参数空间与之前的LoRA以及基础模型的关键方向保持正交。这从数学上保证了新任务的遗忘不会干扰旧任务的保留。
2. **分布外检测器（OOD Detector）：**
   - 为了在推理时决定是否启用遗忘LoRA，O3 训练了一个基于对比熵损失（Contrastive Entropy Loss）的OOD检测器。
   - 当输入被检测为属于“遗忘领域”时，激活相应的LoRA进行干扰或拒绝；当输入为通用领域时，旁路LoRA，直接使用基础模型。

**解决的问题：**

- **参数解耦：** 解决了持续遗忘中的干扰问题（Interference）。
- **无数据依赖：** 不需要访问原始训练数据（Retain Set）来防止灾难性遗忘，这对于保护数据隐私至关重要。
- **效果：** 在TOFU和ScienceQA上的实验表明，O3 是目前唯一能在连续处理多个遗忘请求后，依然保持模型初始效用不下降的框架 35。

------



## 7. 大模型遗忘的新前沿：推理与逻辑（Reasoning Models）



随着OpenAI o1、DeepSeek-R1等推理模型（Reasoning Models）的兴起，遗忘的焦点开始从“事实记忆”转向“推理链条”（Chain-of-Thought, CoT）。



### 7.1 R-TOFU与推理链遗忘



传统的遗忘评估（如TOFU）仅关注模型最终答案是否正确（例如，问“哈利波特是谁？”，模型答“不知道”即算成功）。然而，2025年的研究指出，模型可能在中间的推理步骤（CoT）中泄露了信息，或者通过特定的解码策略（如ZeroThink）绕过遗忘 38。

R-TOFU基准：

这是一个专门针对推理模型的遗忘基准。它不仅评估答案（Answer-level），还评估推理痕迹（Trace-level）。

Reasoned IDK（理性的“我不知道”）：

针对推理模型，简单的拒绝（直接输出“I don't know”）会破坏模型的推理连贯性。Reasoned IDK 是一种新的偏好优化策略，它训练模型生成一段连贯的推理过程，最终逻辑推导至“无法回答”的结论。

- *例子：* “关于X的信息，经过检索我的知识库，并没有发现相关记录，因此我无法回答。”
- 相比于生硬的拒绝，Reasoned IDK 在保留模型的推理能力（Reasoning Utility）方面表现更好，避免了模型因强行截断推理而导致的智力下降 38。

------



## 8. 评估体系与基准测试



2024-2025年，该领域建立了一套标准化的评估体系，以量化遗忘效果。



### 8.1 TOFU (Task of Fictitious Unlearning)



**TOFU** 8 是目前最权威的基准。

- **数据集：** 包含200个完全虚构的作者及其传记。由于是虚构数据，确保了模型仅在微调阶段见过这些知识，排除了预训练数据的干扰。
- **核心指标：**
  - **Forget Quality (FQ)：** 通过比较遗忘后模型与“重新训练模型（Retrained Model）”在遗忘集上的概率分布。FQ越接近1，说明遗忘越彻底且精准。
  - **Model Utility：** 在标准QA数据集或保留的虚构作者上的准确率。



### 8.2 MUSE (Machine Unlearning Six-Way Evaluation)



**MUSE** 41 批评了TOFU仅关注QA的局限性，提出了六维评估体系：

1. **逐字记忆（Verbatim Memorization）：** 能否背诵原文。
2. **知识记忆（Knowledge Memorization）：** 能否回答改写后的问题。
3. **隐私泄露（Privacy Leakage）：** 抵抗成员推理攻击（MIA）的能力。
4. **效用保留（Utility Preservation）。**
5. **可扩展性（Scalability）：** 遗忘集增大时的表现。
6. **可持续性（Sustainability）：** 连续遗忘的表现。

- **重要发现：** 大多数算法（如NPO）能通过前两项测试，但在**隐私泄露**上往往不及格，说明模型深层参数中仍残留有统计学特征 42。



### 8.3 WMDP (Weapons of Mass Destruction Proxy)



**WMDP** 3 关注**安全性遗忘**。

- **内容：** 生物安全、网络安全、化学安全领域的危险知识。
- **目标：** 将模型在这些危险问题上的准确率降低到随机水平，同时保持在通用生物学或计算机科学上的能力。CUT（WMPD配套的遗忘算法）展示了通过遗忘消除潜在危害的可行性。

------



## 9. 核心结论与技术展望



下表总结了当前主流方法的特性对比：

| **方法类别** | **代表算法**           | **核心机制**                       | **优势**                                 | **劣势**                                         |
| ------------ | ---------------------- | ---------------------------------- | ---------------------------------------- | ------------------------------------------------ |
| **优化类**   | **NPO / SimNPO**       | 负向偏好学习，视遗忘数据为拒绝样本 | 稳定性好，TOFU基准SOTA，SimNPO效率更高   | 计算成本中等，NPO存在参考模型偏差                |
| **表示类**   | **RMU / Adaptive RMU** | 导向激活向量至随机/无关空间        | 计算极快，通用效用保留极好，防越狱       | 存在“遗忘痕迹”，易被重学习攻击恢复，本质是“掩盖” |
| **参数高效** | **O3 Framework**       | 正交LoRA + OOD检测                 | 支持持续遗忘，无需访问保留数据，保护隐私 | 推理时需要OOD模块介入，增加系统复杂度            |
| **算术类**   | **Task Vectors / STA** | 权重向量减法                       | 无需训练（Zero-shot），即插即用          | 容易误伤相关能力，需预先获取任务向量             |



### 总结与展望



调研显示，LLM遗忘技术在2024-2025年取得了长足进步。**SimNPO** 和 **Adaptive RMU** 分别在优化和表示层面达到了新的高度，而 **O3** 框架则为实际部署中的持续遗忘提供了可行路径。

然而，**深层挑战依然存在**：

1. **遗忘痕迹（Unlearning Traces）：** 几乎所有方法（尤其是RMU类）都被证明可以通过少量的样本微调（Relearning Attack）快速恢复记忆。这表明当前技术更多是在**隐藏**知识，而非物理**擦除** 19。
2. **推理链的隐蔽性：** 随着推理模型的发展，如何在不破坏逻辑链条的前提下剔除特定推理路径（如R-TOFU所关注的），是2025年的研究热点 38。
3. **噪声数据下的鲁棒性：** 现实中的遗忘请求往往是不精确的、含噪的。如何在“噪声遗忘集”上实现鲁棒遗忘（如2所述）是走向实用化的必经之路。

综上所述，LLM遗忘已从早期的破坏性微调，演变为一个包含优化理论、表示工程和系统架构设计的精密学科。未来的突破将依赖于对模型记忆机制更底层的可解释性研究，以实现真正的“机器脑科手术”。