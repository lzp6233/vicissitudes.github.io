# DEPN：预训练语言模型中隐私神经元的检测与编辑 - 深度分析报告

## 1. 引言：大语言模型时代的机器遗忘 (Machine Unlearning) 势在必行

预训练语言模型（Pretrained Language Models, PLMs）的崛起彻底改变了人工智能的格局，赋予了系统前所未有的流畅度、推理能力和语义理解力。BERT、RoBERTa 以及 GPT 系列等模型的效能归功于对海量数据集的摄取，而这些数据往往是从开放网络上不加区分地抓取的。这种以数据为中心的范式虽然在技术上具有变革性，但也引入了一个严重的漏洞：对敏感信息的意外记忆。当这些模型为了最小化训练语料的困惑度（Perplexity）而优化参数时，它们不可避免地在神经权重中编码了个人身份信息（PII）——包括姓名、电话号码、电子邮件地址和物理位置。随之而来的数据泄露风险，即模型在特定提示下回吐隐私数据，构成了严重的隐私侵犯，也是生成式 AI 伦理部署的重大障碍。

随着《欧盟通用数据保护条例》（GDPR）和《加州消费者隐私法案》（CCPA）等严格数据保护法规的出台，“被遗忘权”（Right to be Forgotten）已被编入法典。这一法律框架规定，个人有权要求从数字系统中删除其个人数据。对于传统数据库而言，这是一个简单的执行命令。然而，对于深度学习模型来说，“删除”是一个非微不足道的挑战。数据并非存储在离散的、可寻址的行中，而是全息地分布在数十亿个参数中。因此，**机器遗忘（Machine Unlearning）**——即从训练好的模型中消除特定数据点影响的过程——已成为一个至关重要的研究前沿。

解决这一问题的传统方法主要分为两个极端：通过重新训练实现的精确遗忘，以及通过差分隐私或微调实现的近似遗忘。对于大语言模型（LLMs）而言，每次收到删除请求都从头开始重新训练在计算上是不可行的，在环境上也是不可持续的。相反，差分隐私方法虽然在理论上是合理的，但往往会征收严重的“隐私-效用税”，即为了保护特定数据点而降低模型的整体性能。

在此背景下，Wu 等人（EMNLP 2023）提出的 **DEPN（Detecting and Editing Privacy Neurons，检测与编辑隐私神经元）** 框架代表了向“手术式”遗忘的范式转变。DEPN 假设隐私信息并非均匀分布，而是局限在 Transformer 架构的前馈神经网络（FFN）中的特定“隐私神经元”内。基于此假设，DEPN 提出了一种使用基于梯度的归因方法来检测这些神经元，并通过激活剪枝（Activation Pruning）对其进行编辑的方法。本报告对 DEPN 框架进行了详尽的分析，剖析了其理论基础、数学机制、在 Enron 基准测试上的实验效果，以及其对隐私保护 AI 未来的广泛影响。

---

## 2. 理论基础：记忆的解剖学

要理解 DEPN 的机制，必须解构其针对的 Transformer 模型架构组件，以及关于这些组件如何存储信息的理论理解。DEPN 框架建立在前馈网络（FFN）层的特定功能角色和神经激活的可解释性之上。

### 2.1 Transformer 架构与 FFN 模块

Transformer 架构是现代 NLP 的支柱，由交替的多头自注意力（MHSA）层和前馈网络（FFN）层组成。虽然人们普遍理解注意力机制负责处理信息的上下文路由（决定哪些 token 应该“关注”哪些 token），但最近的机械可解释性（Mechanistic Interpretability）研究阐明了 FFN 层的独特作用。

Transformer 块中的 FFN 通常是一个两层感知机（MLP），它独立处理每个 token 位置。它通过扩展和投影过程转换隐藏状态 $x$（来自注意力层和层归一化的输出）：

$$\text{FFN}(x) = \sigma(x W_1 + b_1) W_2 + b_2$$

其中：

- $x \in \mathbb{R}^d$ 是输入向量。
- $W_1 \in \mathbb{R}^{d \times d_{ff}}$ 和 $b_1 \in \mathbb{R}^{d_{ff}}$ 是第一线性层的权重和偏置，通常用于扩维（$d_{ff} \approx 4d$）。
- $\sigma$ 是非线性激活函数，如 ReLU、GeLU 或 Swish。
- $W_2 \in \mathbb{R}^{d_{ff} \times d}$ 和 $b_2 \in \mathbb{R}^d$ 是第二线性层的参数，用于投影回嵌入维度。

### 2.2 键-值记忆假设 (Key-Value Memory Hypothesis)

支撑 DEPN 的基础理论是 FFN 的 **键-值记忆（Key-Value Memory）** 解释，该理论由 Geva 等人 (2021) 和 Dai 等人 (2022) 推广。该假设认为 FFN 层充当关联记忆，存储静态知识，包括事实数据和语言模式。

在这个概念框架中：

1. **第一层（$W_1$）作为键（Keys）：** 第一权重矩阵 $W_1$ 的行向量充当模式检测器。它们有效地扫描输入表示 $x$ 中的特定特征或文本模式。当输入 $x$ 与权重向量中编码的特定模式对齐时，隐藏层中的对应神经元（在激活 $\sigma$ 之后）会以高值被激活（Firing）。
2. **第二层（$W_2$）作为值（Values）：** 第二权重矩阵 $W_2$ 的列向量代表与这些模式相关的输出分布。当一个神经元被激活时，它将其对应的“值”向量从 $W_2$ 贡献到残差流中，从而提升输出词汇分布中的特定 token。

对隐私的启示：

DEPN 将这一“知识神经元”假设扩展到了隐私领域。如果 FFN 存储了像“巴黎是法国的首都”这样的事实，那么它们可以说也存储了像“John Smith 的邮箱是 john.smith@enron.com”这样的隐私关联。该框架将 隐私神经元 定义为 FFN 中的那些特定单元，当它们被包含 PII 的上下文激活时，会显著增加模型生成敏感隐私 token 的概率。

这种局部化假设至关重要。如果隐私信息是全息地分布在所有参数中（就像全息图中每一块碎片都包含整个图像一样），那么在不破坏模型的情况下进行手术式移除将是不可能的。DEPN 的成功依赖于特定记忆的 **稀疏性** 和 **局部性**——即电话号码不是由整个网络编码的，而是由一个可发现的神经元子集编码的。

### 2.3 多义性 (Polysemanticity) 的挑战

机械可解释性中的一个反向概念是 **多义性**，或称“叠加假设”（Superposition Hypothesis）。这表明，由于模型相对于人类概念的浩瀚而言容量有限，单个神经元通常编码多个不相关的特征。一个神经元可能同时响应“猫的图像”、“关于金融的文本”和“语法否定”。

DEPN 必须应对这一挑战。如果被识别为存储电话号码的“隐私神经元”对于一般的数字推理或句法结构也至关重要，那么擦除它（将其激活设置为零）可能会降低模型的一般效用。该框架的方法论，特别是积分梯度的使用和通过困惑度指标的验证，旨在识别那些 *唯一地* 或 *主要地* 归因于隐私痕迹的神经元，从而最大限度地减少对模型一般能力的附带损害。

---

## 3. 方法论：DEPN 框架架构

DEPN 框架被构建为一个由三个不同模块组成的后处理流水线：**隐私神经元检测器**、**隐私神经元编辑器**和**隐私神经元聚合器**。这种模块化设计使其能够应用于任何基于 Transformer 的预训练模型，而无需更改原始训练目标或访问完整的训练语料库。

### 3.1 模块 I：隐私神经元检测器

检测器的主要目标是量化 FFN 中每个神经元对生成特定隐私 token 的贡献。为了实现这一点，DEPN 采用了 **积分梯度（Integrated Gradients, IG）**，这是一种公理化归因方法，解决了标准梯度技术的缺点。

#### 3.1.1 简单梯度的局限性

基于标准梯度的敏感性分析计算输出概率相对于神经元激活的梯度：$\frac{\partial P(y|x)}{\partial n}$。虽然计算成本低廉，但该方法受到 **饱和问题** 的困扰。在深度神经网络中，一个神经元可能处于“饱和”状态（maxed out）。在这种状态下，输入的微小变化不会改变输出（梯度为零），但该神经元对功能至关重要。使用简单梯度会错误地将零重要性归因于这些高度活跃的关键神经元。

#### 3.1.2 积分梯度公式

积分梯度通过沿一条从 **基线**（通常为零向量，代表神经元不存在）到实际输入状态的线性路径累积梯度来解决饱和问题。

对于激活值为 $a_n$ 的特定神经元 $n$ 和基线激活 $a'_n = 0$，关于目标隐私输出 $y$ 的积分梯度归因 $Attr(n)$定义为：

$$ Attr(n) = (a_n - 0) \times \int_{\alpha=0}^{1} \frac{\partial P(y|x, n=\alpha \cdot a_n)}{\partial n} d\alpha $$

在 DEPN 的离散实现中，该积分使用具有 $m$ 个步长的黎曼和来近似：

$$ Attr(n) \approx a_n \times \frac{1}{m} \sum_{k=1}^{m} \frac{\partial P(y|x)}{\partial n} \Big|_{n = \frac{k}{m} a_n} $$

其中：

- $P(y|x)$ 是模型在给定上下文 $x$ 的情况下分配给隐私 token $y$ 的概率。
- $m$ 是决定近似粒度的超参数。GitHub 实现建议对 $m$ 进行特定调整以平衡精度和计算时间。
- 计算过程涉及将神经元的激活从 0 缩放到其实际值，分 $m$ 步进行，并平均每一步观察到的梯度。

此过程为模型中的每个神经元分配一个 **隐私归因分数（Privacy Attribution Score）**。具有高正分数的神经元被视为“隐私神经元”——它们的激活积极推动模型生成隐私数据。

### 3.2 模块 II：隐私神经元编辑器

一旦识别出风险神经元，编辑器模块将执行遗忘操作。DEPN 利用了一种“硬掩码”（Hard Masking）技术，在功能上等同于剪枝。

#### 3.2.1 擦除机制

对于识别出的前 $k$ 个隐私神经元集合 $\mathcal{N}_{priv}$，编辑器构建一个二进制掩码 $M$。在模型的前向传播过程中，FFN 层的激活被修改如下：

$$\text{FFN}_{edited}(x) = (M \odot \sigma(x W_1 + b_1)) W_2 + b_2$$

其中，如果神经元 $i \in \mathcal{N}_{priv}$，则 $M_i = 0$，否则 $M_i = 1$。实际上，被识别的神经元被永久静音。无论输入如何，它们的激活都被强制为零。

#### 3.2.2 超参数敏感性：擦除阈值

要擦除的神经元数量（$k$）是控制隐私与效用之间权衡的关键超参数。

- **擦除不足：** 如果编辑的神经元太少，模型可能会“绕过”损伤，使用冗余路径仍然表达隐私信息（在剪枝中称为 Hydra 效应）。
- **过度擦除：** 如果编辑的神经元太多，模型会丧失一般语言理解能力，导致非隐私文本上的困惑度飙升。

对实施细节的分析揭示了基于隐私数据 *类型* 的推荐阈值范围：

- **特定实体（电话号码、姓名）：** 由于这些事实的稀疏性，**10-200 个神经元** 的较紧范围通常就足够了。
- **非结构化文本：** 对于更广泛的文本序列，可能需要 **200-500 个神经元** 的范围。

### 3.3 模块 III：隐私神经元聚合器

在生产环境中，隐私请求很少是单一的。系统管理员可能需要同时处理数千个“被遗忘权”请求。天真地按顺序为每个请求运行检测器和编辑器是低效的，并且存在冲突编辑的风险，即一个擦除操作抵消了另一个操作的效用保留。

**隐私神经元聚合器** 促进了批处理。它通过同时计算一批隐私样本的归因分数，并聚合这些分数来识别在该批次中具有全局风险的神经元。

#### 3.3.1 聚合逻辑

聚合器计算联合归因分数，可能是对批次样本中的个体积分梯度分数进行求和或平均：

$$Attr_{batch}(n) = \sum_{i=1}^{B} Attr(n, x_i, y_i)$$

这识别出了“枢纽神经元”（Hub Neurons）——这些神经元涉及多种不同隐私信息的泄露。修剪这些枢纽可以获得很高的投资回报：一次编辑可以缓解多种隐私风险。

聚合过程涉及复杂的集合论考虑（风险神经元的交集与并集）。框架使用比率超参数来平衡这一点，实证结果表明，特定的比率乘积值（约 0.005）在擦除有效性和模型稳定性之间产生了最佳平衡。

---

## 4. 实验分析与验证

DEPN 的有效性在隐私领域使用标准基准进行了严格评估，并与机器遗忘和差分隐私中的既定基线进行了对比。

### 4.1 实验设置：Enron 基准

用于验证的主要数据集是 **Enron 电子邮件数据集**。该语料库是 NLP 隐私研究的 *事实标准*，因为它源于安然公司（Enron Corporation）员工生成的超过 500,000 封电子邮件，这些邮件在联邦调查该公司倒闭期间向公众发布。

**数据特征：**

- **PII 密度：** 该数据集富含现实世界的 PII，包括姓名、公司电子邮件地址、手机号码以及敏感商务会议的讨论。
- **预处理：** 对于 DEPN 实验，数据经过预处理以掩盖敏感实体并创建（上下文，目标）对，其中上下文是 PII 实体之前的电子邮件文本，目标是 PII 本身（例如，电话号码）。

模型架构：

实验主要使用 BERT-base 和 BERT-large 模型。这些是仅编码器（Encoder-only）架构，通过掩码语言建模（MLM）进行训练。选择 BERT 可以与之前关于知识神经元的工作进行直接比较，尽管它在现代 LLM 的生成性质方面存在局限性（在第 6 节讨论）。

### 4.2 评估指标

为了评估遗忘过程的成功与否，采用了两个相互竞争的指标：

1. **攻击成功率 (Attack Success Rate, ASR) / 隐私泄露：**
   - 该指标测量模型在给定上下文提示时正确预测隐私 token 的概率。
   - *目标：* 最小化 ASR。理想情况下，编辑后模型的 ASR 在统计上应与从未见过该数据的模型无法区分。
   - 计算涉及使用特定隐私上下文探测模型，并检查目标 token 的排名或概率。
2. **困惑度 (Perplexity, PPL)：**
   - 该指标测量模型在保留的 *一般、非隐私* 文本验证集（例如，维基百科文章或 BookCorpus）上的不确定性。
   - *目标：* 维持 PPL。困惑度的显著增加表明“脑损伤”——模型失去了一般的语言能力。
   - 困惑度定义为序列的平均负对数似然的指数。低困惑度意味着模型为真实文本分配高概率。

### 4.3 比较结果

实验结果突显了 DEPN 与基线相比在隐私与效用的帕累托前沿上的优越地位。

#### 4.3.1 DEPN vs. 微调 (Fine-tuning, FT)

微调试图通过在非隐私数据上训练来“覆盖”隐私知识。

- **结果：** 虽然 FT 降低了 ASR，但它通常需要大量的计算（在大型语料库上重新训练）以避免灾难性遗忘。如果 FT 数据集太小，模型会过拟合，一般 PPL 会飙升。
- **DEPN 优势：** DEPN 实现了类似或更好的 ASR 降低，同时对 PPL 的影响微乎其微，且计算成本仅为 FT 的一小部分，因为它只修改极小部分的参数（激活），而不是更新所有权重。

#### 4.3.2 DEPN vs. 差分隐私 (Differential Privacy, DP)

DP-SGD（差分隐私随机梯度下降）在训练期间添加噪声。

- **结果：** DP 保证了隐私界限，但显著降低了最终模型的质量（高 PPL）。此外，DP 必须在预训练 *期间* 应用。
- **DEPN 优势：** DEPN 是一种事后（post-hoc）方法。它可以“治愈”一个在没有隐私约束下已经训练好的“患病”模型，保留原始训练的高效用。

#### 4.3.3 DEPN vs. 知识神经元 (Knowledge Neuron, KN) 方法

改编自编辑事实知识（例如，“罗马在意大利”）的方法专注于编辑特定事实。

- **结果：** 虽然有效，但标准 KN 方法通常难以应对隐私数据的特殊性质，因为隐私数据可能比百科全书式的事实结构性更差。
- **DEPN 优势：** 通过使用积分梯度而不是简单的激活相关性，DEPN 提供了对特定隐私痕迹更鲁棒的定位，从而实现更干净的擦除。

**实验结果总结表**

| **方法论**             | **隐私擦除 (ASR)** | **效用保留 (PPL)** | **计算效率**  | **事后适用性** |
| ---------------------- | ------------------ | ------------------ | ------------- | -------------- |
| **重新训练 (精确)**    | 高                 | 高                 | 极低          | 否             |
| **微调 (Fine-tuning)** | 中                 | 低 (遗忘风险)      | 中            | 是             |
| **差分隐私 (DP)**      | 高                 | 低                 | 低 (训练更慢) | 否             |
| **DEPN (本文提出)**    | **高**             | **高**             | **高**        | **是**         |

*表 1：源自 DEPN 实验结果的比较性能分析。*

---

## 5. 讨论：对隐私机制的洞察

DEPN 研究的发现为大语言模型如何组织信息提供了深刻的见解，将讨论扩展到了简单的工程解决方案之外。

### 5.1 隐私信息的局部性

DEPN 的成功为深度学习中的 **局部性假设（Locality Hypothesis）** 提供了强有力的实证证据。它表明隐私信息并非不可分割地编织在神经网络的整个结构中，而是存储在离散的、可识别的区域中。

- **类比：** 模型的运作方式不太像汤（成分不可分割地混合在一起），而更像图书馆（特定的书籍包含特定的信息）。DEPN 充当图书管理员，从书架上移除特定的书籍。
- **层分布：** 研究表明这些隐私神经元并非随机分布。虽然摘要细节在确切的层分布上很少，但典型的可解释性发现表明，低层处理句法和形态，而高层处理语义和事实。据推断，隐私神经元集中在 Transformer 的中上层，那里是“知识”整合的地方。

### 5.2 “枢纽神经元”现象

来自聚合器实验的一个有趣见解是存在“枢纽神经元”（Hub Neurons）或“风险神经元”，它们参与了 *多个* 不同隐私实体的记忆。

- **解释：** 这些神经元可能编码诸如“电话号码模式”或“电子邮件结构”之类的语义类别。当模型试图回忆特定的电话号码时，它会激活这些类别特定的神经元以及特定的值神经元。
- **战略价值：** 针对这些枢纽可以实现高效的批量遗忘。禁用“电话号码”神经元可能会降低模型输出 *任何* 电话号码的能力，这在隐私意识应用中是一个特性，而不是错误。这与保留一般知识的目标（我们希望保留“城市”的概念但删除“巴黎”）形成对比，凸显了隐私编辑与知识编辑的不同目标。

### 5.3 稀疏性作为一种安全特性

实验结果表明，擦除相对较少数量的神经元（数百万个中的 10-500 个）足以破坏隐私痕迹。这种 **稀疏性** 是有利的。这意味着隐私记忆是“脆弱的”——它们依赖于狭窄的激活路径。相比之下，一般语言能力（语法、句法）是“鲁棒的”，依赖于冗余的、分布式的表示。这种结构存储上的差异使得 DEPN 能够像手术刀一样，在不损害鲁棒语言能力的情况下去除脆弱的隐私数据。

---

## 6. 实施指南与工程最佳实践

对于寻求在生产环境中部署 DEPN 的从业者，随附的 GitHub 存储库提供了实施蓝图。以下指南综合了复现所需的技术细节。

### 6.1 先决条件与环境

- **框架：** 实现依赖于 PyTorch 和 Hugging Face `transformers` 库。
- **数据格式：** 隐私数据必须整理成包含上下文（输入文本）和目标隐私 token 的特定 JSON 格式。
  - *示例：* `{"text": "Contact John at", "target": "555-0199"}`。
- **模型支持：** 基线代码支持 BERT 架构（`bert-base-uncased`, `bert-large-uncased`）。适应其他架构（如 RoBERTa, GPT）需要修改脚本中的层访问器。

### 6.2 三步执行工作流

步骤 1：记忆检测

在遗忘之前，必须验证模型实际知道什么。并非训练集中的所有隐私数据都被记住了。

- **操作：** 运行 `bash run_get_memorization.sh`。
- **输出：** 一个过滤后的数据点列表，其中模型为隐私 token 分配了高概率。试图“遗忘”模型未记住的数据在计算上是浪费的。

**步骤 2：神经元检测（归因）**

- **操作：** 执行积分梯度脚本。
- **配置：**
  - 将 `m`（黎曼步长）设置为至少 20，以平衡速度和准确性。较高的值（如 50）提供更好的归因，但速度较慢。
  - 选择目标层范围（通常是所有 FFN 层）。

**步骤 3：编辑与聚合**

- **操作：** 将掩码应用于识别出的神经元。
- **超参数调整：**
  - **`erase_kn_num`**：根据目标数据类型设置此整数值。
    - 对于高度特定的实体（ID、电话号码），使用 **10-200**。
    - 对于较宽松的语义文本，或者如果第一遍后 ASR 仍然很高，使用 **200-500**。
  - **聚合器比率：** 如果处理批次，调整聚合比率，使两个比率参数的乘积近似为 **0.005**。这个经验性的“魔术数字”被发现可以优化批次中风险神经元的交集。

### 6.3 验证

编辑后，运行验证套件至关重要：

1. **验证擦除：** 检查目标数据上的 ASR。它应接近于零。
2. **健全性检查：** 在标准基准（例如 GLUE 或仅 WikiText 上的困惑度）上运行模型，以确保模型没有被破坏。

---

## 7. 局限性与未来方向

尽管 DEPN 代表了重大进步，但它并非没有局限性。对框架的批判性分析揭示了未来研究必须解决的具体挑战。

### 7.1 多 Token 语义单元挑战

当前归因方法（包括 IG）的一个主要结构限制是它们专注于单 token 输出。

- **问题：** 隐私信息的语义单元通常跨越多个 token。例如，“New York”是两个 token；根据分词器的不同，电话号码可能被分成三到四个 token。
- **DEPN 约束：** DEPN 计算 *单个* 目标 token 的归因。如果隐私风险是序列“John Smith”，而模型仅计算“Smith”的归因，它可能会错过负责预测“John”的神经元。
- **影响：** 这需要复杂的“滑动窗口”归因或多 token 聚合策略来完全擦除多词实体，这使流水线复杂化。

### 7.2 对仅解码器（生成式）模型的可扩展性

DEPN 的核心实验集中在 BERT，一种仅编码器模型。

- **问题：** 生成式模型（GPT-4, Llama）是仅解码器且自回归的。神经元在步骤 $t$ 的激活依赖于 $t-1$ 的激活。这种时间依赖性使得归因比 BERT 的双向上下文复杂得多。
- **未来工作：** GitHub 存储库暗示了在 **Llama-2-7B** 上的正在进行的工作，表明 DEPN 的原则是可转移的，但可能需要修改归因公式以考虑因果掩码和自回归生成。

### 7.3 隐私攻击的军备竞赛

机器遗忘是一个对抗性领域。虽然 DEPN 成功降低了标准提示的 ASR，但尚不清楚信息是真正消失了还是仅仅被抑制了。

- **风险：** 高级攻击（例如，软提示调整或基于梯度的反演）可能能够绕过归零的神经元，在注意力层或偏置项中找到记忆的“幽灵痕迹”。仅靠神经元剪枝很难证明真正的“信息论”删除。

### 7.4 联邦学习集成

未来应用的一个令人兴奋的途径是 **联邦遗忘（Federated Unlearning）**。在数据分布在客户端设备（例如手机）上的场景中，客户端可能会请求删除数据。DEPN 理论上可以适应于在设备上本地识别隐私神经元，并将“神经元掩码”更新传输到全局模型，从而在不向中央服务器暴露用户数据的情况下实现隐私合规。

---

## 8. 结论

DEPN（检测与编辑隐私神经元）是 AI 安全领域日益成熟的一个复杂证明。通过超越完全重新训练和差分隐私等钝器，它提供了一种尊重的大语言模型复杂内部结构的 **手术式数据保护方法**。

该框架的核心创新——应用积分梯度来定位充当隐私数据键-值记忆的 FFN 神经元——为后处理训练模型提供了理论依据和实践可行的方法。其在 Enron 基准上减少隐私泄露（ASR）同时保持一般效用（PPL）的证明能力，验证了神经记忆的“局部性假设”，并为 AI 时代的监管合规提供了蓝图。

然而，前进的道路并非没有障碍。从编码器到解码器架构的过渡、多 token 实体的处理以及对抗性提取的鲁棒性仍然是开放的挑战。尽管如此，DEPN 建立了一个关键的基线。它将叙事从囤积秘密的“黑盒”模型转变为“灰盒”系统，在其中特定的记忆可以被审计、定位和切除。随着模型规模和普及率的持续增长，这种细粒度的、神经元级别的控制机制对于确保人工智能在不损害隐私的情况下服务于人类将是必不可少的。

---

**参考文献源自对以下内容的分析：**

- **主要论文:** Wu et al., EMNLP 2023.
- **方法论:** Integrated Gradients, Knowledge Neurons.
- **实验:** Enron Dataset, Metrics (Perplexity, ASR).
- **实现:** GitHub Repository.
- **背景:** Machine Unlearning & Privacy.